{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P3_WrangleData\n",
    "\n",
    "## [CSV File ](#jump1)\n",
    "#### read method\n",
    "1. 普通\n",
    "        open文件 as f object;  \n",
    "        header = f.readline() 读一条记录  \n",
    "        for loop 遍历\n",
    "        输出结果 data [{col1:value1,col2:value2},{row2},{row3}]  \n",
    "\n",
    "2. csv module \n",
    "        csv.reader()   \n",
    "        csv.DicReader(f)\n",
    "\n",
    "#### write  method 同上\n",
    "\n",
    "## [XLS File](#jump2)\n",
    "### read method\n",
    "1. 生成sheet对象   \n",
    "        import xlrd  \n",
    "        workbook = xlrd.open_workbook(datafile)  \n",
    "        sheet = workbook.sheet_by_index(0)  \n",
    "2. 可以转换成二维数组：   \n",
    "        data = [[sheet.cell_value(r, col)   \n",
    "            for col in range(sheet.ncols)]   \n",
    "                for r in range(sheet.nrows)]  \n",
    "3. other useful methods：  \n",
    "        sheet.nrows\n",
    "        sheet.cell_type(3,2)\n",
    "        sheet.cell_value(3,2)\n",
    "        sheet.col_values(3, start_rowx=1, end_rowx=4)\n",
    "\n",
    "## [JSON File](#jump3)\n",
    "1. JSON形式，类似字典，value中有更多的嵌套[{},{}]\n",
    "\n",
    "2. 可以调用网站的API获取JSON格式的数据  \n",
    "        r = request.get(url,params)  \n",
    "        r.json()  \n",
    "        r.text  \n",
    "        r.url  \n",
    "不知道想要的数据应该怎么传入到参数中，可能网站API有使用说明\n",
    "3. dic <-> json\n",
    "\n",
    "## [XML](#jump4)\n",
    "有且只有一个root\n",
    "1. ET method \n",
    "        import xml.etree.ElementTree /xml.etree.cElementTree module(快) as ET    \n",
    "        解析tree  \n",
    "        getroot   \n",
    "        root.find(xpath)->return element\n",
    "        ET.iterparse 分批加载解析\n",
    "\n",
    "2. BeautifulSoup method\n",
    "        from bs4 import BeautifulSoup    \n",
    "        soup = BeautifulSoup(xmlfile)  \n",
    "        soup.find(tag,attr)->return element  \n",
    "\n",
    "## Data Quality\n",
    "Validity: 格式，数值范围等符合要求  \n",
    "    1. 查看各个字段的数据类型\n",
    "    2. 某个字段的数值种类及分布--BluePrint\n",
    "Accuracy  \n",
    "Completeness  \n",
    "Consistency  \n",
    "Unifomity: 统一单位  \n",
    "\n",
    "## [MongoDB](#jump6)\n",
    "1. pymongo: translate a python dictinoary into BSON \n",
    "   连接 MongDB 和 Python\n",
    "2. find query/projection; Insert document; update; save; remove\n",
    "3. Operators:\n",
    "    Range; Exists; Regex; query array; Dot notation\n",
    "4. aggregation pipeline 可进行多步运算\n",
    "    [mongodb_aggregation_operator](https://docs.mongodb.com/manual/reference/operator/aggregation-pipeline/)  \n",
    "5. Index\n",
    "6. start MongoDB\n",
    "\n",
    "\n",
    "## [Case Study:Open Street Map](#jump7)\n",
    "1. Familiarize with data\n",
    "2. Iterative Parsing\n",
    "3. Explore Tags of ways(event)\n",
    "    探究element.tag的类型;  \n",
    "    element k属性的值的 错误格式类型\n",
    "4. Auditing & Improving Street Names\n",
    "5. Data Model for Database\n",
    "    改变数据格式（模型），存入mongodb\n",
    "\n",
    "\n",
    "\n",
    "## [附录](#jump5)\n",
    "#### josn data <--> python data\n",
    "#### 内部编码转换\n",
    "#### 解压文件\n",
    "#### 连接路径\n",
    "#### 输出某路径下文件\n",
    "#### pprint 优美的输出\n",
    "#### Re 正则表达式\n",
    "#### request module\n",
    "#### Best Practice For Scraping\n",
    "#### defaultdict \n",
    "#### SET用法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"jump1\">\n",
    "## CSV File </span>\n",
    "### read csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def parse_file(datafile):\n",
    "    data = []\n",
    "    \n",
    "    with open(datafile, \"r\") as f:\n",
    "        header = f.readline().split(',')\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            if count == 10:\n",
    "                break\n",
    "            fields = line.split(',')\n",
    "            entry = {}\n",
    "            for i,value in enumerate(fields):\n",
    "                entry[header[i].strip()] = value.strip()\n",
    "            data.append(entry)\n",
    "            count = count+1\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pprint  #pprint.pprint  优美的输出\n",
    "#import os\n",
    "def parse_csv(datafile):\n",
    "    data = []\n",
    "    count = 0\n",
    "    with open(datafile ,'rb') as sd:\n",
    "        r = csv.DictReader(sd)\n",
    "        for line in r :\n",
    "            if count == 10:\n",
    "                break\n",
    "            data.append(line)\n",
    "            count = count+1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATADIR = \"/Users/weidian1/Documents/Study/nanodegreee/P3\"\n",
    "DATAFILE = \"beatles-diskography.csv\"\n",
    "\n",
    "def test(DATADIR,DATAFILE,parse_f):\n",
    "    datafile = os.path.join(DATADIR, DATAFILE)\n",
    "    d = parse_f(datafile)\n",
    "    firstline = {'Title': 'Please Please Me', 'UK Chart Position': '1', 'Label': 'Parlophone(UK)', 'Released': '22 March 1963', 'US Chart Position': '-', 'RIAA Certification': 'Platinum', 'BPI Certification': 'Gold'}\n",
    "    tenthline = {'Title': '', 'UK Chart Position': '1', 'Label': 'Parlophone(UK)', 'Released': '10 July 1964', 'US Chart Position': '-', 'RIAA Certification': '', 'BPI Certification': 'Gold'}\n",
    "    #print d[0]\n",
    "    assert d[0] == firstline\n",
    "    assert d[9] == tenthline\n",
    "\n",
    "    \n",
    "test(DATADIR,DATAFILE,parse_f=parse_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('eggs.csv', 'wb') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter=',')\n",
    "                            #,quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    spamwriter.writerow(['Spam'] * 5 + ['Baked Beans'])\n",
    "    spamwriter.writerow(['Spam', 'Lovely Spam', 'Wonderful Spam'])\n",
    "    spamwriter.writerow(['FAR_WEST']+ ['Max Load','Year'])\n",
    "    \n",
    "#exapmle outfile\n",
    "#Spam,Spam,Spam,Spam,Spam,Baked Beans\n",
    "#Spam,Lovely Spam,Wonderful Spam\n",
    "#FAR_WEST,Max Load,Year \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"jump2\">\n",
    "## XLS File </span>\n",
    "### read xls file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List Comprehension\n",
      "data[3][2]: 1036.088697\n",
      "\n",
      "Cells in a nested loop:\n",
      "41277.0833333 9238.73731 1438.20528 1565.442856 916.708348 14010.903488 3027.98334 6165.211119 1157.741663 37520.933404 \n",
      "ROWS, COLUMNS, and CELLS:\n",
      "Number of rows in the sheet: 7296\n",
      "Type of data in cell (row 3, col 2): 2\n",
      "Value in cell (row 3, col 2): 1036.088697\n",
      "Get a slice of values in column 3, from rows 1-3:\n",
      "[1411.7505669999982, 1403.4722870000019, 1395.053150000001]\n",
      "\n",
      "DATES:\n",
      "Type of data in cell (row 1, col 0): 3\n",
      "Time in Excel format: 41275.0416667\n",
      "Convert time to a Python datetime tuple, from the Excel float: (2013, 1, 1, 1, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "#pip install xlrd\n",
    "import xlrd\n",
    "datedir = \"/Users/weidian1/Documents/Study/nanodegreee/P3\"\n",
    "datafile = \"2013_ERCOT_Hourly_Load_Data.xls\"\n",
    "\n",
    "\n",
    "def parse_file(datedir,datafile):\n",
    "    data = os.path.join(datedir, datafile)\n",
    "    workbook = xlrd.open_workbook(data)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "\n",
    "    data = [[sheet.cell_value(r, col) \n",
    "                for col in range(sheet.ncols)] \n",
    "                    for r in range(sheet.nrows)]\n",
    "\n",
    "    print \"\\nList Comprehension\"\n",
    "    print \"data[3][2]:\",\n",
    "    print data[3][2]\n",
    "\n",
    "    print \"\\nCells in a nested loop:\"    \n",
    "    for row in range(sheet.nrows):\n",
    "        for col in range(sheet.ncols):\n",
    "            if row == 50:\n",
    "                print sheet.cell_value(row, col),\n",
    "\n",
    "\n",
    "    ### other useful methods:\n",
    "    print \"\\nROWS, COLUMNS, and CELLS:\"\n",
    "    print \"Number of rows in the sheet:\", \n",
    "    print sheet.nrows\n",
    "    print \"Type of data in cell (row 3, col 2):\", \n",
    "    print sheet.cell_type(3, 2)\n",
    "    print \"Value in cell (row 3, col 2):\", \n",
    "    print sheet.cell_value(3, 2)\n",
    "    print \"Get a slice of values in column 3, from rows 1-3:\"\n",
    "    print sheet.col_values(3, start_rowx=1, end_rowx=4)\n",
    "\n",
    "    print \"\\nDATES:\"\n",
    "    print \"Type of data in cell (row 1, col 0):\", \n",
    "    print sheet.cell_type(1, 0)\n",
    "    exceltime = sheet.cell_value(1, 0)\n",
    "    print \"Time in Excel format:\",\n",
    "    print exceltime\n",
    "    print \"Convert time to a Python datetime tuple, from the Excel float:\",\n",
    "    print xlrd.xldate_as_tuple(exceltime, 0)\n",
    "\n",
    "    return data\n",
    "\n",
    "data = parse_file(datedir,datafile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"jump3\">\n",
    "## JSON File </span>\n",
    "### Get JSON fomart data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requesting http://musicbrainz.org/ws/2/artist/?query=artist%3ANirvana&fmt=json\n",
      "{\n",
      "    \"artists\": [\n",
      "        {\n",
      "            \"aliases\": [\n",
      "                {\n",
      "                    \"begin-date\": null, \n",
      "                    \"end-date\": null, \n",
      "                    \"locale\": null, \n",
      "                    \"name\": \"Nirvana US\", \n",
      "                    \"primary\": null, \n",
      "                    \"sort-name\": \"Nirvana US\", \n",
      "                    \"type\": null\n",
      "                }\n",
      "            ], \n",
      "            \"area\": {\n",
      "                \"id\": \"489ce91b-6658-3307-9877-795b68554c98\", \n",
      "                \"name\": \"United States\", \n",
      "                \"sort-name\": \"United States\"\n",
      "            }, \n",
      "            \"begin-area\": {\n",
      "                \"id\": \"a640b45c-c173-49b1-8030-973603e895b5\", \n",
      "                \"name\": \"Aberdeen\", \n",
      "                \"sort-name\": \"Aberdeen\"\n",
      "            }, \n",
      "            \"country\": \"US\", \n",
      "            \"disambiguation\": \"90s US grunge band\", \n",
      "            \"id\": \"5b11f4ce-a62d-471e-81fc-a69a8278c7da\", \n",
      "            \"life-span\": {\n",
      "                \"begin\": \"1988-01\", \n",
      "                \"end\": \"1994-04-05\", \n",
      "                \"ended\": true\n",
      "            }, \n",
      "            \"name\": \"Nirvana\", \n",
      "            \"score\": \"100\", \n",
      "            \"sort-name\": \"Nirvana\", \n",
      "            \"tags\": [\n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"punk\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"legendary\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"90\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"seattle\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"northwest\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"alternative\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"rock and indie\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"usa\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"am\\u00e9ricain\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"united states\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"kurt cobain\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"90s\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 4, \n",
      "                    \"name\": \"alternative rock\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"band\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 13, \n",
      "                    \"name\": \"grunge\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 9, \n",
      "                    \"name\": \"rock\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"acoustic rock\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"noise rock\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"nirvana\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 5, \n",
      "                    \"name\": \"american\"\n",
      "                }\n",
      "            ], \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"8a754a16-0027-3a29-b6d7-2b40ea0481ed\", \n",
      "                \"name\": \"United Kingdom\", \n",
      "                \"sort-name\": \"United Kingdom\"\n",
      "            }, \n",
      "            \"begin-area\": {\n",
      "                \"id\": \"f03d09b3-39dc-4083-afd6-159e3f0d462f\", \n",
      "                \"name\": \"London\", \n",
      "                \"sort-name\": \"London\"\n",
      "            }, \n",
      "            \"country\": \"GB\", \n",
      "            \"disambiguation\": \"60s band from the UK\", \n",
      "            \"id\": \"9282c8b4-ca0b-4c6b-b7e3-4f7762dfc4d6\", \n",
      "            \"life-span\": {\n",
      "                \"begin\": \"1967\", \n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Nirvana\", \n",
      "            \"score\": \"100\", \n",
      "            \"sort-name\": \"Nirvana\", \n",
      "            \"tags\": [\n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"rock\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"pop\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"progressive rock\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"orchestral\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"british\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"power pop\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"psychedelic rock\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"soft rock\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"symphonic rock\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"english\"\n",
      "                }\n",
      "            ], \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"6a264f94-6ff1-30b1-9a81-41f7bfabd616\", \n",
      "                \"name\": \"Finland\", \n",
      "                \"sort-name\": \"Finland\"\n",
      "            }, \n",
      "            \"country\": \"FI\", \n",
      "            \"disambiguation\": \"Early 1980's Finnish punk band\", \n",
      "            \"id\": \"85af0709-95db-4fbc-801a-120e9f4766d0\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Nirvana\", \n",
      "            \"score\": \"100\", \n",
      "            \"sort-name\": \"Nirvana\", \n",
      "            \"tags\": [\n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"punk\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"finland\"\n",
      "                }\n",
      "            ], \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"disambiguation\": \"French band from Martigues, activ during the 70s.\", \n",
      "            \"id\": \"c49d69dc-e008-47cf-b5ff-160fafb1fe1f\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Nirvana\", \n",
      "            \"score\": \"100\", \n",
      "            \"sort-name\": \"Nirvana\"\n",
      "        }, \n",
      "        {\n",
      "            \"disambiguation\": \"founded in 1987 by a Michael Jackson double/imitator\", \n",
      "            \"id\": \"3aa878c0-224b-41e5-abd1-63be359d2bca\", \n",
      "            \"life-span\": {\n",
      "                \"begin\": \"1987\", \n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Nirvana\", \n",
      "            \"score\": \"100\", \n",
      "            \"sort-name\": \"Nirvana\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"b305320e-c158-43f4-b5be-4450e2f99a32\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"El Nirvana\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Nirvana, El\"\n",
      "        }, \n",
      "        {\n",
      "            \"aliases\": [\n",
      "                {\n",
      "                    \"begin-date\": null, \n",
      "                    \"end-date\": null, \n",
      "                    \"locale\": null, \n",
      "                    \"name\": \"Nirvana\", \n",
      "                    \"primary\": null, \n",
      "                    \"sort-name\": \"Nirvana\", \n",
      "                    \"type\": null\n",
      "                }, \n",
      "                {\n",
      "                    \"begin-date\": null, \n",
      "                    \"end-date\": null, \n",
      "                    \"locale\": null, \n",
      "                    \"name\": \"Prophet 2002\", \n",
      "                    \"primary\": null, \n",
      "                    \"sort-name\": \"Prophet 2002\", \n",
      "                    \"type\": null\n",
      "                }\n",
      "            ], \n",
      "            \"area\": {\n",
      "                \"id\": \"23d10872-f5ae-3f0c-bf55-332788a16ecb\", \n",
      "                \"name\": \"Sweden\", \n",
      "                \"sort-name\": \"Sweden\"\n",
      "            }, \n",
      "            \"country\": \"SE\", \n",
      "            \"disambiguation\": \"Swedish death metal band\", \n",
      "            \"id\": \"f2dfdff9-3862-4be0-bf85-9c833fa3059e\", \n",
      "            \"life-span\": {\n",
      "                \"begin\": \"1988\", \n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Nirvana 2002\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Nirvana 2002\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"329c04ae-3b73-4ca3-996f-75608ab1befb\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Nirvana Singh\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Singh, Nirvana\", \n",
      "            \"type\": \"Person\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"489ce91b-6658-3307-9877-795b68554c98\", \n",
      "                \"name\": \"United States\", \n",
      "                \"sort-name\": \"United States\"\n",
      "            }, \n",
      "            \"country\": \"US\", \n",
      "            \"id\": \"c3a64a25-251b-4d03-afba-1471440245b8\", \n",
      "            \"life-span\": {\n",
      "                \"begin\": \"2009\", \n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Approaching Nirvana\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Approaching Nirvana\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"489ce91b-6658-3307-9877-795b68554c98\", \n",
      "                \"name\": \"United States\", \n",
      "                \"sort-name\": \"United States\"\n",
      "            }, \n",
      "            \"country\": \"US\", \n",
      "            \"gender\": \"female\", \n",
      "            \"id\": \"206419e0-3a7a-49ce-8437-4e757767d02b\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Nirvana Savoury\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Savoury, Nirvana\", \n",
      "            \"type\": \"Person\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"86f9ae24-ba2a-4d55-9275-0b89b85f6e3a\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Weed Nirvana\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Weed Nirvana\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"e8ad73e9-9e7f-41c4-a395-6e29260ff1df\", \n",
      "                \"name\": \"Graz\", \n",
      "                \"sort-name\": \"Graz\"\n",
      "            }, \n",
      "            \"begin-area\": {\n",
      "                \"id\": \"e8ad73e9-9e7f-41c4-a395-6e29260ff1df\", \n",
      "                \"name\": \"Graz\", \n",
      "                \"sort-name\": \"Graz\"\n",
      "            }, \n",
      "            \"disambiguation\": \"Nirvana-Coverband\", \n",
      "            \"id\": \"46d8dae4-abec-438b-9c62-a3dbb2aaa1b7\", \n",
      "            \"life-span\": {\n",
      "                \"begin\": \"2000\", \n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Nirvana Teen Spirit\", \n",
      "            \"score\": \"50\", \n",
      "            \"sort-name\": \"Nirvana Teen Spirit\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"c621114d-73cc-4832-8afe-f13dc261e5af\", \n",
      "                \"name\": \"Gatineau\", \n",
      "                \"sort-name\": \"Gatineau\"\n",
      "            }, \n",
      "            \"begin-area\": {\n",
      "                \"id\": \"c621114d-73cc-4832-8afe-f13dc261e5af\", \n",
      "                \"name\": \"Gatineau\", \n",
      "                \"sort-name\": \"Gatineau\"\n",
      "            }, \n",
      "            \"id\": \"02c4e6bb-7b7a-4686-8c23-df01bfd42b0e\", \n",
      "            \"life-span\": {\n",
      "                \"begin\": \"2012-04-05\", \n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Sappy Nirvana Tribute\", \n",
      "            \"score\": \"50\", \n",
      "            \"sort-name\": \"Sappy Nirvana Tribute\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"e1388435-f80d-434a-9980-f1c9f5aa9b90\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Nirvana Sitar & String Group\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"Nirvana Sitar & String Group\"\n",
      "        }\n",
      "    ], \n",
      "    \"count\": 14, \n",
      "    \"created\": \"2016-09-22T03:46:46.648Z\", \n",
      "    \"offset\": 0\n",
      "}\n",
      "\n",
      "ARTIST:\n",
      "{\n",
      "    \"area\": {\n",
      "        \"id\": \"8a754a16-0027-3a29-b6d7-2b40ea0481ed\", \n",
      "        \"name\": \"United Kingdom\", \n",
      "        \"sort-name\": \"United Kingdom\"\n",
      "    }, \n",
      "    \"begin-area\": {\n",
      "        \"id\": \"f03d09b3-39dc-4083-afd6-159e3f0d462f\", \n",
      "        \"name\": \"London\", \n",
      "        \"sort-name\": \"London\"\n",
      "    }, \n",
      "    \"country\": \"GB\", \n",
      "    \"disambiguation\": \"60s band from the UK\", \n",
      "    \"id\": \"9282c8b4-ca0b-4c6b-b7e3-4f7762dfc4d6\", \n",
      "    \"life-span\": {\n",
      "        \"begin\": \"1967\", \n",
      "        \"ended\": null\n",
      "    }, \n",
      "    \"name\": \"Nirvana\", \n",
      "    \"score\": \"100\", \n",
      "    \"sort-name\": \"Nirvana\", \n",
      "    \"tags\": [\n",
      "        {\n",
      "            \"count\": 1, \n",
      "            \"name\": \"rock\"\n",
      "        }, \n",
      "        {\n",
      "            \"count\": 1, \n",
      "            \"name\": \"pop\"\n",
      "        }, \n",
      "        {\n",
      "            \"count\": 1, \n",
      "            \"name\": \"progressive rock\"\n",
      "        }, \n",
      "        {\n",
      "            \"count\": 1, \n",
      "            \"name\": \"orchestral\"\n",
      "        }, \n",
      "        {\n",
      "            \"count\": 1, \n",
      "            \"name\": \"british\"\n",
      "        }, \n",
      "        {\n",
      "            \"count\": 1, \n",
      "            \"name\": \"power pop\"\n",
      "        }, \n",
      "        {\n",
      "            \"count\": 1, \n",
      "            \"name\": \"psychedelic rock\"\n",
      "        }, \n",
      "        {\n",
      "            \"count\": 1, \n",
      "            \"name\": \"soft rock\"\n",
      "        }, \n",
      "        {\n",
      "            \"count\": 1, \n",
      "            \"name\": \"symphonic rock\"\n",
      "        }, \n",
      "        {\n",
      "            \"count\": 1, \n",
      "            \"name\": \"english\"\n",
      "        }\n",
      "    ], \n",
      "    \"type\": \"Group\"\n",
      "}\n",
      "requesting http://musicbrainz.org/ws/2/artist/9282c8b4-ca0b-4c6b-b7e3-4f7762dfc4d6?fmt=json&inc=releases\n",
      "\n",
      "ONE RELEASE:\n",
      "{\n",
      "  \"barcode\": null, \n",
      "  \"country\": \"GB\", \n",
      "  \"date\": \"1969\", \n",
      "  \"disambiguation\": \"\", \n",
      "  \"id\": \"0b44cb36-550a-491d-bfd9-8751271f9de7\", \n",
      "  \"packaging\": null, \n",
      "  \"packaging-id\": null, \n",
      "  \"quality\": \"normal\", \n",
      "  \"release-events\": [\n",
      "    {\n",
      "      \"area\": {\n",
      "        \"disambiguation\": \"\", \n",
      "        \"id\": \"8a754a16-0027-3a29-b6d7-2b40ea0481ed\", \n",
      "        \"iso-3166-1-codes\": [\n",
      "          \"GB\"\n",
      "        ], \n",
      "        \"name\": \"United Kingdom\", \n",
      "        \"sort-name\": \"United Kingdom\"\n",
      "      }, \n",
      "      \"date\": \"1969\"\n",
      "    }\n",
      "  ], \n",
      "  \"status\": \"Official\", \n",
      "  \"status-id\": \"4e304316-386d-3409-af2e-78857eec5cfe\", \n",
      "  \"text-representation\": {\n",
      "    \"language\": \"eng\", \n",
      "    \"script\": \"Latn\"\n",
      "  }, \n",
      "  \"title\": \"To Markos III\"\n",
      "}\n",
      "\n",
      "ALL TITLES:\n",
      "To Markos III\n",
      "Travelling on a Cloud\n",
      "Songs Of Love And Praise\n",
      "Songs of Love and Praise\n",
      "Songs of Love and Praise\n",
      "Secret Theatre\n",
      "The Story of Simon Simopath\n",
      "Me And My Friend\n",
      "All of Us\n",
      "The Story of Simon Simopath\n",
      "To Markos III\n",
      "Chemistry\n",
      "Local Anaesthetic\n",
      "Orange & Blue\n",
      "Pentecost Hotel\n",
      "Black Flower\n",
      "All of Us\n"
     ]
    }
   ],
   "source": [
    "# To experiment with this code freely you will have to run this code locally.\n",
    "# Take a look at the main() function for an example of how to use the code.\n",
    "# We have provided example json output in the other code editor tabs for you to\n",
    "# look at, but you will not be able to run any queries through our UI.\n",
    "import json\n",
    "import requests\n",
    "\n",
    "\n",
    "BASE_URL = \"http://musicbrainz.org/ws/2/\"\n",
    "ARTIST_URL = BASE_URL + \"artist/\"\n",
    "\n",
    "# query parameters are given to the requests.get function as a dictionary; this\n",
    "# variable contains some starter parameters.\n",
    "query_type = {  \"simple\": {},\n",
    "                \"atr\": {\"inc\": \"aliases+tags+ratings\"},\n",
    "                \"aliases\": {\"inc\": \"aliases\"},\n",
    "                \"releases\": {\"inc\": \"releases\"}}\n",
    "\n",
    "\n",
    "def query_site(url, params, uid=\"\", fmt=\"json\"):\n",
    "    # This is the main function for making queries to the musicbrainz API.\n",
    "    # A json document should be returned by the query.\n",
    "    params[\"fmt\"] = fmt\n",
    "    r = requests.get(url + uid, params=params)\n",
    "    print \"requesting\", r.url\n",
    "\n",
    "    #检测响应状态码\n",
    "    if r.status_code == requests.codes.ok:  \n",
    "        return r.json()\n",
    "    else:\n",
    "        r.raise_for_status()\n",
    "\n",
    "\n",
    "def query_by_name(url, params, name):\n",
    "    # This adds an artist name to the query parameters before making\n",
    "    # an API call to the function above.\n",
    "    params[\"query\"] = \"artist:\" + name\n",
    "    return query_site(url, params)\n",
    "\n",
    "\n",
    "def pretty_print(data, indent=4):\n",
    "    # After we get our output, we can format it to be more readable\n",
    "    # by using this function.\n",
    "    if type(data) == dict:\n",
    "        print json.dumps(data, indent=indent, sort_keys=True)\n",
    "    else:\n",
    "        print data\n",
    "\n",
    "\n",
    "def main():\n",
    "    '''\n",
    "    Modify the function calls and indexing below to answer the questions on\n",
    "    the next quiz. HINT: Note how the output we get from the site is a\n",
    "    multi-level JSON document, so try making print statements to step through\n",
    "    the structure one level at a time or copy the output to a separate output\n",
    "    file.\n",
    "    '''\n",
    "    results = query_by_name(ARTIST_URL, query_type[\"simple\"], \"Nirvana\")\n",
    "    pretty_print(results)\n",
    "\n",
    "    artist_id = results[\"artists\"][1][\"id\"]\n",
    "    print \"\\nARTIST:\"\n",
    "    pretty_print(results[\"artists\"][1])\n",
    "\n",
    "    artist_data = query_site(ARTIST_URL, query_type[\"releases\"], artist_id)\n",
    "    releases = artist_data[\"releases\"]\n",
    "    print \"\\nONE RELEASE:\"\n",
    "    pretty_print(releases[0], indent=2)\n",
    "    release_titles = [r[\"title\"] for r in releases]\n",
    "\n",
    "    print \"\\nALL TITLES:\"\n",
    "    for t in release_titles:\n",
    "        print t\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"jump4\">\n",
    "## XML File </span>\n",
    "### Xml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API  \n",
    "[XPath_sytax](http://www.w3schools.com/xsl/xpath_syntax.asp)  \n",
    "[xml.etree.elementtree](https://docs.python.org/2/library/xml.etree.elementtree.html)  \n",
    "[BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Title:\n",
      "Standardization of the functional syndesmosis widening by dynamic U.S examination\n",
      "\n",
      "            \n",
      "\n",
      "Author email addresses:\n",
      "omer@extremegate.com\n",
      "mcarmont@hotmail.com\n",
      "laver17@gmail.com\n",
      "nyska@internet-zahav.net\n",
      "kammarh@gmail.com\n",
      "gideon.mann.md@gmail.com\n",
      "barns.nz@gmail.com\n",
      "eukots@gmail.com\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pprint\n",
    "import codecs\n",
    "\n",
    "tree = ET.parse(codecs.open(\"exampleResearchArticle.xml\", encoding=\"UTF-8\"))\n",
    "root = tree.getroot()\n",
    "\n",
    "title = root.find('./fm/bibl/title')\n",
    "title_text = \"\"\n",
    "for p in title:\n",
    "    title_text += p.text\n",
    "print \"\\nTitle:\\n\",title_text\n",
    "\n",
    "print \"\\nAuthor email addresses:\"\n",
    "for a in root.findall('./fm/bibl/aug/au'):\n",
    "    email = a.find('email')\n",
    "    if email is not None:\n",
    "        print email.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# 搜索框中的选项  [{名：缩写},...]\n",
    "def options(soup, id):\n",
    "    option_values = []\n",
    "    carrier_list = soup.find(id = id)\n",
    "    #soup.find(\"a\", id=\"link3\")     tag='a',attribution id='link3'\n",
    "    for option in carrier_list.find_all('option'): #tag 是 option的所有element\n",
    "        option_values.append(option['value']) #响应的 value属性的值\n",
    "    return option_values\n",
    "\n",
    "#格式化输出 print(\"I'm %s. I'm %d year old\" % ('Vamei', 99))\n",
    "def print_list(label, codes):\n",
    "    print \"\\n%s:\"%label\n",
    "    for c in codes:\n",
    "        print c\n",
    "\n",
    "#提交表单， \n",
    "def post():        \n",
    "    #保留会话，get和post用一个会话\n",
    "    s = requests.Session()\n",
    "\n",
    "    r = s.get(\"http://www.transtats.bts.gov/Data_Elements.aspx?Data=2\")  \n",
    "    soup = BeautifulSoup(r.text)\n",
    "    viewstate = soup.find('input',id='__VIEWSTATE')['value']\n",
    "    eventvalidation = soup.find('input',id='__EVENTVALIDATION')['value']\n",
    "\n",
    "    r = s.post(\"http://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "                  data = {\"__EVENTTARGET\":\"\",\n",
    "                          \"__EVENTARGUMENT\":\"\",\n",
    "                          \"__VIEWSTATE\":viewstate,\n",
    "                          \"__EVENTVALIDATION\":eventvalidation,\n",
    "                          \"CarrierList\":CarrierList['Virgin America'],\n",
    "                          \"AirportList\":AirportList[' - Boston, MA: Logan International'],\n",
    "                          \"Submit\":'Submit'})\n",
    "def write_html(file_name):\n",
    "    with open(file_name,\"w\") as f:\n",
    "        f.write(r.text)\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # BeautifulSoup(markup, \"html.parser\")  parser library：‘lxml’，‘html.parser’ ...\n",
    "    soup = BeautifulSoup(open(\"virgin_and_logan_airport.html\"))\n",
    "    CarrierList = options(soup,'CarrierList')\n",
    "    #print_list(\"Carries\",codes)\n",
    "    CarrierList = options(soup,'AirportList')\n",
    "    #print_list(\"Airpirts\",codes)\n",
    "    post()\n",
    "    write_html(\"virgin_and_logan_airport_out.html\")\n",
    "    \n",
    "    \n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# \n",
    "import xml.etree.cElementTree as ET\n",
    "#dict subclass that calls a factory function to supply missing values。\n",
    "#defaultdict(default_factory), 按default_factory给key生成默认的value\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "osm_file = open(\"chicago.osm\", \"r\")\n",
    "\n",
    "#设置匹配模式：非空白字符，以1个或0个‘.’结束， 忽略大小写\n",
    "street_type_re = re.compile(r'\\S+\\.?$', re.IGNORECASE)\n",
    "street_types = defaultdict(int) #默认value=0\n",
    "\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "\n",
    "        street_types[street_type] += 1\n",
    "\n",
    "def print_sorted_dict(d):\n",
    "    keys = d.keys()\n",
    "    keys = sorted(keys, key=lambda s: s.lower())\n",
    "    for k in keys:\n",
    "        v = d[k]\n",
    "        print \"%s: %d\" % (k, v)\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.tag == \"tag\") and (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "def audit():\n",
    "    #ET.iterparse 不一次全部加载进内存，\n",
    "    for event, elem in ET.iterparse(osm_file):\n",
    "        if is_street_name(elem):\n",
    "            audit_street_type(street_types, elem.attrib['v'])\n",
    "    print_sorted_dict(street_types)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    audit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"jump6\">\n",
    "## MongoDB </span>\n",
    "\n",
    "### 1. PyMongo\n",
    "PyMongo translate a python dictinoary into BSON  \n",
    "[PyMongo_doc](https://docs.mongodb.com/getting-started/python/query/)  \n",
    "[PyMongo](http://api.mongodb.com/python/2.8/tutorial.html#bulk-inserts)  \n",
    "[mongodb_manual](https://docs.mongodb.com/manual/reference/command/insert/#dbcmd.insert)  \n",
    "[Query_Project](https://docs.mongodb.com/manual/reference/operator/query/)\n",
    "\n",
    "### 2. Find/Insert/Update/Save/Remove\n",
    "Methods:get data into MongoDB  \n",
    "1. cleaning data in python, then put into MongoDB  \n",
    "2. Json data,then bulk import MongoDB (using mongoimport)  \n",
    "mongoimport -db dbname -c collectionname --file input-file.json  \n",
    "[mongoimport](https://docs.mongodb.com/manual/reference/program/mongoimport/)  \n",
    "ps:If no hostname and credentials are supplied, mongoimport will try to connect to the default localhost:27017\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"插入数据docment  [PyMongo translate a python dictinoary into BSON encoding and send it across to the database]\n",
    "Insert a document into a collection named autos.\n",
    "The operation will create the collection if the collection does not currently exist.\n",
    "\"\"\"\n",
    "from pymongo import MongoClient\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client['example']\n",
    "tesla_s = {}\n",
    "db.autos.insert(tesla_s) #autos 是一个 collection\n",
    "num_autos = db.autos.find().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"字段筛选\n",
    "\"\"\"\n",
    "\n",
    "# Do not edit code below this line in the online code editor.\n",
    "# Code here is for local use on your own computer.\n",
    "def get_db(db_name):\n",
    "    # For local use\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client[db_name]\n",
    "    return db\n",
    "\n",
    "def find_porsche(db, query):\n",
    "    # For local use\n",
    "    return db.autos.find({\"manufacturer\":\"Porsche\",\"class\":\"mid-size car\"})\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # For local use\n",
    "    db = get_db('examples')\n",
    "    query = porsche_query()\n",
    "    results = find_porsche(db, query)\n",
    "\n",
    "    print \"Printing first 3 results\\n\"\n",
    "    import pprint\n",
    "    for car in results[:3]:\n",
    "        pprint.pprint(car)\n",
    "        \n",
    "# find everything\n",
    "db.nodes.find().pretty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#projection 是在query查询结果之后的筛选，query类似where限制条件，projection类似select 的选择字段\n",
    "def find():\n",
    "    query ={\"manufacturer\":\"Porsche\",\"class\":\"mid-size car\"}\n",
    "    projection ={\"_id\":0,\"name\":1} #不返回id，返回name\n",
    "    result = db.autos.find(query,projection)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updates Methods: \n",
    "1. db.cities.save(city)  \n",
    "2. city = db.cities.update({\"name\":\"Munchen\",\"country\":\"Germany\"},{\"$set\":{\"isoCountryCode\":\"DEU\"}}) \n",
    "\n",
    "3. city = db.cities.update({\"name\":\"Munchen\",\"country\":\"Germany\"}, {\"$unset\":{\"isoCountryCode\":\"\"}})  \n",
    "\n",
    "4. city = db.cities.update({\"country\":\"Germany\"},{\"$set\":{\"isoCountryCode\":\"DEU\"}},multi = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'db' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-48402f74f750>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"isoCountryCode\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"DEU\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcity\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#可以指定 _id(新增/更新)/系统分配\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'db' is not defined"
     ]
    }
   ],
   "source": [
    "#save\n",
    "city = db.cities.find_one({})\n",
    "city[\"isoCountryCode\"] = \"DEU\"\n",
    "db.cities.save(city)  #可以指定 _id(新增/更新)/系统分配\n",
    "\n",
    "#update  $set\n",
    "city = db.cities.update({\"name\":\"Munchen\",\"country\":\"Germany\"},{\"$set\":{\"isoCountryCode\":\"DEU\"}}) #无isoCountryCode：新增/有：更新\n",
    "\n",
    "#update $unset 有：删除/无：不修改\n",
    "\n",
    "#multi update \n",
    "city = db.cities.update({\"country\":\"Germany\"},{\"$set\":{\"isoCountryCode\":\"DEU\"}},multi = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#DON‘T DO THIS  符合查询条件的docment 全部替换成 {\"isoCountryCode\":\"DEU\"}\n",
    "city = db.cities.update({\"name\":\"Munchen\",\"country\":\"Germany\"},{\"isoCountryCode\":\"DEU\"}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db.cities.remove() \n",
    "db.cities.remove({\"name\":\"Chicago\"}) #删除满足条件的documents\n",
    "db.cities.drop() #删除整个collection和相关的元数据，比如索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. operator\n",
    "1. range operator\n",
    "2. if field exists \n",
    "3. regex operator    \n",
    "    Live RegEx tester at [regexpal.com](http://www.regexpal.com/)    \n",
    "    MongoDB [$regex Manual](https://docs.mongodb.com/manual/reference/operator/query/regex/)  \n",
    "    Official Python [Regular Expression HOWTO](https://docs.python.org/2/howto/regex.html)  \n",
    "    [Another good Python Regular Expressions page](https://developers.google.com/edu/python/regular-expressions?csw=1)  \n",
    "4. in / all operator\n",
    "5. query array结构化数据 using scalars标量(like,int,strings)\n",
    "6. Dot notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. aggregation pipeline ：operate one by one\n",
    "[mongodb_aggregation_operator](https://docs.mongodb.com/manual/reference/operator/aggregation-pipeline/)  \n",
    "\n",
    "operation|fuction\n",
    "--|--|\n",
    "\\$group| divide\n",
    "\\$sort  |sort\n",
    "\\$project  |shaping: pass specified fields(existing or newly computed) to the next stage.  \n",
    "\\$match |filtering\n",
    "\\$skip  |skip top\n",
    "\\$limit  |get top\n",
    "\\$unwind 可以将一个document按array中不同的value分解成多个documents  \n",
    "\n",
    "[group operators](https://docs.mongodb.com/manual/reference/operator/aggregation/group/)  \n",
    "\n",
    "\\$sum  \n",
    "\\$first  \n",
    "\\$last  \n",
    "\\$max  \n",
    "\\$min  \n",
    "\\$avg  \n",
    "\\$push  \n",
    "\\$addToSet  \n",
    "\n",
    "Conditional Aggregation Operators\n",
    "ifNull:null、missing，返回替代值；否则，返回原value\n",
    "\n",
    "\n",
    "     {\n",
    "         \"$project\": {\n",
    "            \"item\": 1,\n",
    "            \"description\": { \"$ifNull\": [ \"$description\", \"Unspecified\" ] }\n",
    "         }\n",
    "      }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#$group;$month等 $表示调用函数\n",
    "# _id;totalPrice;averageQuantity;count等 输出的field\n",
    "#_id 分组的依据，不可省略，可为NULL或固值（在对整体求平均时）\n",
    "#$group  不带order功能\n",
    "db.sales.aggregate(\n",
    "   [\n",
    "      {\n",
    "        \"$group\" : {\n",
    "           \"_id\" : { \"month\": { \"$month\": \"$date\" }, \"day\": { \"$dayOfMonth\": \"$date\" }, \"year\": { \"$year\": \"$date\" } },\n",
    "           \"totalPrice\": { \"$sum\": { \"$multiply\": [ \"$price\", \"$quantity\" ] } },\n",
    "           \"averageQuantity\": { \"$avg\": \"$quantity\" },\n",
    "           \"count\": { \"$sum\": 1 }\n",
    "        }\n",
    "      }\n",
    "      {\n",
    "        \"$sort\" :{ \"averageQuantity\": -1}\n",
    "        }\n",
    "   ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# $ 用于1.表示调用函数 2.取该filed name的value，而非字符串\n",
    "# \"screen_name\":\"$user.screen_name\" 第一个screen_name 表示结果的field name（不存在，则新建）\n",
    "\n",
    "pipeline = [\n",
    "                {\"$match\":{\"user.time_zone\":\"Brasilia\",\n",
    "                           \"user.statuses_count\":{\"$gte\":100},\n",
    "                            \"user.followers_count\":{\"$gt\":0}}},\n",
    "                {\"$project\":{\"screen_name\":\"$user.screen_name\",\n",
    "                             \"tweets\":\"$user.statuses_count\",\n",
    "                             \"followers\":\"$user.followers_count\"}},\n",
    "                {\"$sort\":{\"followers\":-1}},\n",
    "                {\"$limit\":1}\n",
    "                ]\n",
    "#match  multi conditions\n",
    "{\"$match\":{\"country\":\"India\",\"lon\":{\"$gte\":75,\"$let\":80}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    pipeline = [{\"$match\":{\"country\":\"India\"}},\n",
    "                {\"$unwind\":\"$isPartOf\"},\n",
    "                {\"$group\":{\"_id\":\"$isPartOf\",\"count\":{\"$sum\":1}}},\n",
    "                {\"$sort\":{\"count\":-1}},\n",
    "                {\"$limit\":1} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# $addToSet\n",
    "{\n",
    "       $group:\n",
    "         {\n",
    "           _id: { day: { $dayOfYear: \"$date\"}, year: { $year: \"$date\" } },\n",
    "           itemsSold: { $addToSet: \"$item\" }\n",
    "         }\n",
    "     }\n",
    "\n",
    "#output\n",
    "{ \"_id\" : { \"day\" : 46, \"year\" : 2014 }, \"itemsSold\" : [ \"xyz\", \"abc\" ] }\n",
    "{ \"_id\" : { \"day\" : 34, \"year\" : 2014 }, \"itemsSold\" : [ \"xyz\", \"jkl\" ] }\n",
    "{ \"_id\" : { \"day\" : 1, \"year\" : 2014 }, \"itemsSold\" : [ \"abc\" ] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# $push\n",
    "{\n",
    "       $group:\n",
    "         {\n",
    "           _id: { day: { $dayOfYear: \"$date\"}, year: { $year: \"$date\" } },\n",
    "           itemsSold: { $push:  { item: \"$item\", quantity: \"$quantity\" } }\n",
    "         }\n",
    "     }\n",
    "\n",
    "#output\n",
    "{\n",
    "   \"_id\" : { \"day\" : 46, \"year\" : 2014 },\n",
    "   \"itemsSold\" : [\n",
    "      { \"item\" : \"abc\", \"quantity\" : 10 },\n",
    "      { \"item\" : \"xyz\", \"quantity\" : 10 },\n",
    "      { \"item\" : \"xyz\", \"quantity\" : 5 },\n",
    "      { \"item\" : \"xyz\", \"quantity\" : 10 }\n",
    "   ]\n",
    "}\n",
    "{\n",
    "   \"_id\" : { \"day\" : 34, \"year\" : 2014 },\n",
    "   \"itemsSold\" : [\n",
    "      { \"item\" : \"jkl\", \"quantity\" : 1 },\n",
    "      { \"item\" : \"xyz\", \"quantity\" : 5 }\n",
    "   ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Index\n",
    "MongoDB creates a unique index on the _id field  \n",
    "[index](https://docs.mongodb.com/manual/indexes/)   的种类有很多，可以是single field，也可以是compound等  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create index\n",
    "db.nodes.ensureindec()\n",
    "\n",
    "#Geospatial Index:   \n",
    "location:[float(x),float(y)]  \n",
    "record['location'] = location\n",
    "client.osm.nodes.ensure_index([('location', pymongo.GEO2D)]) #GEO2D is a direction argument "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 启动MongoDB\n",
    "\n",
    "shell中：\n",
    "\n",
    "> zouxiaxiaMacBookPro:~ zxx$ mongod  \n",
    "\n",
    "另开一个shell：\n",
    "> zouxiaxiaMacBookPro:~ zxx$ cd /usr/local/Cellar/mongodb/3.2.10/bin   \n",
    "\n",
    "> zouxiaxiaMacBookPro:bin zxx$ ./mongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"jump7\">\n",
    "## Case Study: Open Street Map </span>\n",
    "### 1. Familiarize with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#看文件开头\n",
    "less chicago.osm\n",
    "# 文件概况\n",
    "ls - l chicago.osm\n",
    "# ls - h\n",
    "#2G太大，无法一次读入内存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Iterative Parsing\n",
    "get all tag, parse it one tag at a time ,see tag as an event\n",
    "event 分“start”和“end”；  \n",
    "取tag（event）中的子tag  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "\n",
    "#get all tag, parse it one tag at a time ,see tag as an event,\n",
    "def count_tags(filename):\n",
    "    tags = defaultdict(int)\n",
    "    for event, elem in ET.iterparse(filename):\n",
    "        tags[elem.tag] += 1\n",
    "    return dict(tags)\n",
    "    \n",
    "\n",
    "\n",
    "#取嵌套tag\n",
    "for tag in elem.iter('tag') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Explore tags of ways\n",
    "* type of tags\n",
    "* types and distribution of k(a attribution of tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "\"\"\"\n",
    "Your task is to explore the data a bit more.\n",
    "Before you process the data and add it into your database, you should check the\n",
    "\"k\" value for each \"<tag>\" and see if there are any potential problems.\n",
    "\n",
    "We have provided you with 3 regular expressions to check for certain patterns\n",
    "in the tags. As we saw in the quiz earlier, we would like to change the data\n",
    "model and expand the \"addr:street\" type of keys to a dictionary like this:\n",
    "{\"address\": {\"street\": \"Some value\"}}\n",
    "So, we have to see if we have such tags, and if we have any tags with\n",
    "problematic characters.\n",
    "\n",
    "Please complete the function 'key_type', such that we have a count of each of\n",
    "four tag categories in a dictionary:\n",
    "  \"lower\", for tags that contain only lowercase letters and are valid,\n",
    "  \"lower_colon\", for otherwise valid tags with a colon in their names,\n",
    "  \"problemchars\", for tags with problematic characters, and\n",
    "  \"other\", for other tags that do not fall into the other three categories.\n",
    "See the 'process_map' and 'test' functions for examples of the expected format.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        if lower.match(element.get(\"k\")):\n",
    "            keys[\"lower\"] +=1\n",
    "        elif lower_colon.match(element.get(\"k\")):\n",
    "            keys[\"lower_colon\"] +=1\n",
    "        elif  problemchars.match(element.get(\"k\")):\n",
    "            keys[\"problemchars\"] +=1\n",
    "        else:\n",
    "            keys[\"other\"] +=1   \n",
    "    return keys\n",
    "\n",
    "\n",
    "def process_map(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "    return keys\n",
    "\n",
    "\n",
    "def test():\n",
    "    # You can use another testfile 'map.osm' to look at your solution\n",
    "    # Note that the assertion below will be incorrect then.\n",
    "    # Note as well that the test function here is only used in the Test Run;\n",
    "    # when you submit, your code will be checked against a different dataset.\n",
    "    keys = process_map('example.osm')\n",
    "    pprint.pprint(keys)\n",
    "    assert keys == {'lower': 5, 'lower_colon': 0, 'other': 1, 'problemchars': 1}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4. Auditing & Improving street name\n",
    "* auditing node 和 way event中 street name  \n",
    "* improve street name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your task in this exercise has two steps:\n",
    "\n",
    "- audit the OSMFILE and change the variable 'mapping' to reflect the changes needed to fix \n",
    "    the unexpected street types to the appropriate ones in the expected list.\n",
    "    You have to add mappings only for the actual problems you find in this OSMFILE,\n",
    "    not a generalized solution, since that may and will depend on the particular area you are auditing.\n",
    "- write the update_name function, to actually fix the street name.\n",
    "    The function takes a string with street name as an argument and should return the fixed name\n",
    "    We have provided a simple test so that you see what exactly is expected\n",
    "\"\"\"\n",
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "OSMFILE = \"example.osm\"\n",
    "\n",
    "#match last part of street (street type) \n",
    "# example: \"North Lincoln Ave\" ,get \"Ave\"\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\"]\n",
    "\n",
    "# UPDATE THIS VARIABLE\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"Rd.\": \"Road\",\n",
    "            \"Ave\": \"Avenue\"\n",
    "            }\n",
    "\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        #street name 不在已知的规范表达方式中\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    #pprint.pprint(dict(street_types))\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\n",
    "\n",
    "def update_name(name, mapping):\n",
    "    for key in mapping.keys():\n",
    "        if name.split(\" \")[-1] == key:\n",
    "            #print name,key\n",
    "            name = name.replace(key,mapping[key])\n",
    "    return name\n",
    "\n",
    "\n",
    "def test():\n",
    "    st_types = audit(OSMFILE)\n",
    "    assert len(st_types) == 3\n",
    "    pprint.pprint(dict(st_types))\n",
    "\n",
    "    for st_type, ways in st_types.iteritems():\n",
    "        for name in ways:\n",
    "            better_name = update_name(name, mapping)\n",
    "            print name, \"=>\", better_name\n",
    "            if name == \"West Lexington St.\":\n",
    "                assert better_name == \"West Lexington Street\"\n",
    "            if name == \"Baldwin Rd.\":\n",
    "                assert better_name == \"Baldwin Road\"\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#example.osm\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<osm version=\"0.6\" generator=\"CGImap 0.3.3 (28791 thorn-03.openstreetmap.org)\" copyright=\"OpenStreetMap and contributors\" attribution=\"http://www.openstreetmap.org/copyright\" license=\"http://opendatacommons.org/licenses/odbl/1-0/\">\n",
    " <bounds minlat=\"41.9704500\" minlon=\"-87.6928300\" maxlat=\"41.9758200\" maxlon=\"-87.6894800\"/>\n",
    " <node id=\"261114295\" visible=\"true\" version=\"7\" changeset=\"11129782\" timestamp=\"2012-03-28T18:31:23Z\" user=\"bbmiller\" uid=\"451048\" lat=\"41.9730791\" lon=\"-87.6866303\"/>\n",
    " <node id=\"261114296\" visible=\"true\" version=\"6\" changeset=\"8448766\" timestamp=\"2011-06-15T17:04:54Z\" user=\"bbmiller\" uid=\"451048\" lat=\"41.9730416\" lon=\"-87.6878512\"/>\n",
    " <node id=\"261114299\" visible=\"true\" version=\"5\" changeset=\"8581395\" timestamp=\"2011-06-29T14:14:14Z\" user=\"bbmiller\" uid=\"451048\" lat=\"41.9729565\" lon=\"-87.6939548\"/>\n",
    " <node id=\"261146436\" visible=\"true\" version=\"5\" changeset=\"8581395\" timestamp=\"2011-06-29T14:14:14Z\" user=\"bbmiller\" uid=\"451048\" lat=\"41.9707380\" lon=\"-87.6976025\"/>\n",
    " <node id=\"261147304\" visible=\"true\" version=\"7\" changeset=\"8581395\" timestamp=\"2011-06-29T14:14:15Z\" user=\"bbmiller\" uid=\"451048\" lat=\"41.9740068\" lon=\"-87.6988576\"/>\n",
    " <node id=\"261224274\" visible=\"true\" version=\"5\" changeset=\"8581395\" timestamp=\"2011-06-29T14:14:14Z\" user=\"bbmiller\" uid=\"451048\" lat=\"41.9707656\" lon=\"-87.6938669\"/>\n",
    " <node id=\"293816175\" visible=\"true\" version=\"47\" changeset=\"8448766\" timestamp=\"2011-06-15T16:55:37Z\" user=\"bbmiller\" uid=\"451048\" lat=\"41.9730154\" lon=\"-87.6890403\"/>\n",
    " <node id=\"305896090\" visible=\"true\" version=\"37\" changeset=\"15348240\" timestamp=\"2013-03-13T07:46:29Z\" user=\"Umbugbene\" uid=\"567034\" lat=\"41.9749225\" lon=\"-87.6891198\"/>\n",
    " <node id=\"317636974\" visible=\"true\" version=\"12\" changeset=\"15348240\" timestamp=\"2013-03-13T08:02:56Z\" user=\"Umbugbene\" uid=\"567034\" lat=\"41.9740292\" lon=\"-87.7012430\"/>\n",
    " <node id=\"317636971\" visible=\"true\" version=\"13\" changeset=\"15348240\" timestamp=\"2013-03-13T08:08:01Z\" user=\"Umbugbene\" uid=\"567034\" lat=\"41.9740556\" lon=\"-87.6979712\"/>\n",
    " <node id=\"317637399\" visible=\"true\" version=\"2\" changeset=\"14927972\" timestamp=\"2013-02-05T22:43:49Z\" user=\"Umbugbene\" uid=\"567034\" lat=\"41.9705609\" lon=\"-87.7012048\"/>\n",
    " <node id=\"317637398\" visible=\"true\" version=\"2\" changeset=\"14927972\" timestamp=\"2013-02-05T22:43:49Z\" user=\"Umbugbene\" uid=\"567034\" lat=\"41.9706972\" lon=\"-87.7012109\"/>\n",
    " <node id=\"365214872\" visible=\"true\" version=\"3\" changeset=\"8448766\" timestamp=\"2011-06-15T17:04:54Z\" user=\"bbmiller\" uid=\"451048\" lat=\"41.9731130\" lon=\"-87.6847998\"/>\n",
    " <node id=\"261299091\" visible=\"true\" version=\"6\" changeset=\"8581395\" timestamp=\"2011-06-29T14:14:15Z\" user=\"bbmiller\" uid=\"451048\" lat=\"41.9747482\" lon=\"-87.6988886\"/>\n",
    " <node id=\"261114294\" visible=\"true\" version=\"6\" changeset=\"8448766\" timestamp=\"2011-06-15T17:04:54Z\" user=\"bbmiller\" uid=\"451048\" lat=\"41.9731219\" lon=\"-87.6841979\"/>\n",
    " <node id=\"261210804\" visible=\"true\" version=\"4\" changeset=\"3359748\" timestamp=\"2009-12-13T00:36:09Z\" user=\"woodpeck_fixbot\" uid=\"147510\" lat=\"41.9707217\" lon=\"-87.7000019\"/>\n",
    " <node id=\"261221422\" visible=\"true\" version=\"7\" changeset=\"8581395\" timestamp=\"2011-06-29T14:14:15Z\" user=\"bbmiller\" uid=\"451048\" lat=\"41.9748542\" lon=\"-87.6922652\"/>\n",
    " <node id=\"261221424\" visible=\"true\" version=\"7\" changeset=\"8581395\" timestamp=\"2011-06-29T14:14:15Z\" user=\"bbmiller\" uid=\"451048\" lat=\"41.9758794\" lon=\"-87.6923639\">\n",
    "  <tag k=\"highway\" v=\"traffic_signals\"/>\n",
    " </node>\n",
    "  <node id=\"2406124091\" visible=\"true\" version=\"2\" changeset=\"17206049\" timestamp=\"2013-08-03T16:43:42Z\" user=\"linuxUser16\" uid=\"1219059\" lat=\"41.9757030\" lon=\"-87.6921867\">\n",
    "  <tag k=\"addr:city\" v=\"Chicago\"/>\n",
    "  <tag k=\"addr:housenumber\" v=\"5157\"/>\n",
    "  <tag k=\"addr:postcode\" v=\"60625\"/>\n",
    "  <tag k=\"addr:street\" v=\"North Lincoln Ave\"/>\n",
    "  <tag k=\"amenity\" v=\"restaurant\"/>\n",
    "  <tag k=\"cuisine\" v=\"mexican\"/>\n",
    "  <tag k=\"name\" v=\"La Cabana De Don Luis\"/>\n",
    "  <tag k=\"outdoor_seating\" v=\"no\"/>\n",
    "  <tag k=\"phone\" v=\"1 (773)-271-5176\"/>\n",
    "  <tag k=\"smoking\" v=\"no\"/>\n",
    "  <tag k=\"takeaway\" v=\"yes\"/>\n",
    " </node>\n",
    "  <node id=\"2636084635\" visible=\"true\" version=\"1\" changeset=\"20187349\" timestamp=\"2014-01-25T01:56:10Z\" user=\"linuxUser16\" uid=\"1219059\" lat=\"41.9705219\" lon=\"-87.6900344\">\n",
    "  <tag k=\"addr:city\" v=\"Chicago\"/>\n",
    "  <tag k=\"addr:country\" v=\"US\"/>\n",
    "  <tag k=\"addr:housenumber\" v=\"4874\"/>\n",
    "  <tag k=\"addr:postcode\" v=\"60625\"/>\n",
    "  <tag k=\"addr:state\" v=\"Illinois\"/>\n",
    "  <tag k=\"addr:street\" v=\"N. Lincoln Ave\"/>\n",
    "  <tag k=\"name\" v=\"Matty Ks\"/>\n",
    "  <tag k=\"phone\" v=\"(773)-654-1347\"/>\n",
    "  <tag k=\"shop\" v=\"doityourself\"/>\n",
    "  <tag k=\"source\" v=\"GPS\"/>\n",
    " </node>\n",
    " <node id=\"261198953\" visible=\"true\" version=\"6\" changeset=\"8581395\" timestamp=\"2011-06-29T14:14:13Z\" user=\"bbmiller\" uid=\"451048\" lat=\"41.9707413\" lon=\"-87.6963097\"/>\n",
    " <node id=\"757860928\" visible=\"true\" version=\"2\" changeset=\"5288876\" timestamp=\"2010-07-22T16:16:51Z\" user=\"uboot\" uid=\"26299\" lat=\"41.9747374\" lon=\"-87.6920102\">\n",
    "  <tag k=\"amenity\" v=\"fast_food\"/>\n",
    "  <tag k=\"cuisine\" v=\"sausage\"/>\n",
    "  <tag k=\"name\" v=\"Shelly's Tasty Freeze\"/>\n",
    " </node>\n",
    "  <way id=\"258219703\" visible=\"true\" version=\"1\" changeset=\"20187382\" timestamp=\"2014-01-25T02:01:54Z\" user=\"linuxUser16\" uid=\"1219059\">\n",
    "  <nd ref=\"2636086179\"/>\n",
    "  <nd ref=\"2636086178\"/>\n",
    "  <nd ref=\"2636086177\"/>\n",
    "  <nd ref=\"2636086176\"/>\n",
    "  <tag k=\"highway\" v=\"service\"/>\n",
    " </way>\n",
    " <node id=\"1683602133\" version=\"2\" timestamp=\"2012-03-20T18:56:44Z\" uid=\"634589\" user=\"Jacobs Studios\" changeset=\"11043902\" lat=\"42.1251718\" lon=\"-88.0780576\">\n",
    "    <tag k=\"addr:housename\" v=\"Village Hall\"/>\n",
    "    <tag k=\"addr:housenumber\" v=\"1400\"/>\n",
    "    <tag k=\"addr:postcode\" v=\"60067\"/>\n",
    "    <tag k=\"addr:street\" v=\"Baldwin Rd.\"/>\n",
    "    <tag k=\"amenity\" v=\"townhall\"/>\n",
    "    <tag k=\"name\" v=\"Village Hall\"/>\n",
    " </node>\n",
    " <way id=\"209809850\" visible=\"true\" version=\"1\" changeset=\"15353317\" timestamp=\"2013-03-13T15:58:04Z\" user=\"chicago-buildings\" uid=\"674454\">\n",
    "  <nd ref=\"2199822281\"/>\n",
    "  <nd ref=\"2199822390\"/>\n",
    "  <nd ref=\"2199822392\"/>\n",
    "  <nd ref=\"2199822369\"/>\n",
    "  <nd ref=\"2199822370\"/>\n",
    "  <nd ref=\"2199822284\"/>\n",
    "  <nd ref=\"2199822281\"/>\n",
    "  <tag k=\"addr:housenumber\" v=\"1412\"/>\n",
    "  <tag k=\"addr:street\" v=\"West Lexington St.\"/>\n",
    "  <tag k=\"addr:street:name\" v=\"Lexington\"/>\n",
    "  <tag k=\"addr:street:prefix\" v=\"West\"/>\n",
    "  <tag k=\"addr:street:type\" v=\"Street\"/>\n",
    "  <tag k=\"building\" v=\"yes\"/>\n",
    "  <tag k=\"building:levels\" v=\"1\"/>\n",
    "  <tag k=\"chicago:building_id\" v=\"366409\"/>\n",
    " </way>\n",
    " <relation id=\"1557627\" visible=\"true\" version=\"2\" changeset=\"14326854\" timestamp=\"2012-12-19T05:32:37Z\" user=\"fredr\" uid=\"939355\">\n",
    "  <member type=\"node\" ref=\"1258927212\" role=\"via\"/>\n",
    "  <member type=\"way\" ref=\"110160127\" role=\"from\"/>\n",
    "  <member type=\"way\" ref=\"34073105\" role=\"to\"/>\n",
    "  <tag k=\"restriction\" v=\"only_right_turn\"/>\n",
    "  <tag k=\"type\" v=\"restriction\"/>\n",
    " </relation>\n",
    "</osm>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"jump5\">\n",
    "## 附录 </span>\n",
    "### josn data <--> python data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"json 模块提供了一种很简单的方式来编码和解码JSON数据。 \n",
    "其中两个主要的函数是 json.dumps() 和 json.loads(),要比其他序列化函数库如pickle的接口少得多。 \"\"\"\n",
    "#下面演示如何将一个Python数据结构转换为JSON：\n",
    "import json\n",
    "\n",
    "data = {\n",
    "    'name' : 'ACME',\n",
    "    'shares' : 100,\n",
    "    'price' : 542.23\n",
    "}\n",
    "\n",
    "json_str = json.dumps(data)\n",
    "#下面演示如何将一个JSON编码的字符串转换回一个Python数据结构：\n",
    "data = json.loads(json_str)\n",
    "\n",
    "\n",
    "#如果你要处理的是文件而不是字符串，你可以使用 json.dump() 和 json.load() 来编码和解码JSON数据。例如：\n",
    "# Writing JSON data\n",
    "with open('data.json', 'w') as f:\n",
    "    json.dump(data, f)\n",
    "\n",
    "# Reading data back\n",
    "with open('data.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 内部编码转换\n",
    "当python要做编码转换的时候，会借助于内部的编码，转换过程是这样的：  \n",
    "原有编码 -> 内部编码 -> 目的编码  \n",
    "python的内部是使用unicode来处理的，但是unicode的使用需要考虑的是它的编码格式有两种，一是UCS-2，它一共有65536个码位，另一种是UCS-4，它有2147483648g个\n",
    "\n",
    "输出：若在终端输出到文件，终端默认ascii编码。解决方法：指定输出编码方式【output.write(line.encode('utf-8')) 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65535\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print sys.maxunicode\n",
    "#如果输出的值为65535,那么就是UCS-2,如果输出是1114111就是UCS-4编码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "12 我爱北京\n",
      "12 我爱北京 <type 'unicode'>\n",
      "4 我爱北京 <type 'str'>\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "import codecs, sys\n",
    "\n",
    "print '-'*60\n",
    "# 创建gb2312编码器\n",
    "look  = codecs.lookup(\"gbk\")\n",
    "# 创建utf-8编码器\n",
    "look2 = codecs.lookup(\"utf-8\")\n",
    "\n",
    "a = \"我爱北京\"\n",
    "\n",
    "print len(a), a\n",
    "# 把a编码为内部的unicode, 但为什么方法名为decode呢，我的理解是把gbk的字符串解码为unicode\n",
    "b = look2.decode(a)\n",
    "# 返回的b[0]是数据，b[1]是长度，这个时候的类型是unicode了\n",
    "print b[1], b[0], type(b[0])\n",
    "# 把内部编码的unicode转换为gb2312编码的字符串，encode方法会返回一个字符串类型\n",
    "b2 = look2.encode(b[0])\n",
    "# 发现不一样的地方了吧？转换回来之后，字符串长度由12变为了4! 现在的返回的长度才是真正的字数，原来的是字节数\n",
    "print b2[1], b2[0], type(b2[0])\n",
    "# 虽然上面返回了字数，但并不意味着用len求b2[0]的长度就是4了，仍然还是12，仅仅是codecs.encode会统计字数\n",
    "print len(b2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解压文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#解压 \n",
    "from zipfile import ZipFile\n",
    "def open_zip(datadir,datafile):\n",
    "    data = os.path.join(datadir, datafile)\n",
    "    with ZipFile('{0}.zip'.format(data), 'r') as myzip:\n",
    "        myzip.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 连接路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "DATADIR = \"/Users/weidian1/Documents/Study/nanodegreee/P3\"\n",
    "DATAFILE = \"beatles-diskography.csv\"\n",
    "datafile = os.path.join(DATADIR, DATAFILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出某路径下文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os,sys\n",
    "\n",
    "path =\"/var/www/html/\"\n",
    "dir = os.listdir(path)\n",
    "\n",
    "for file in dirs:\n",
    "    print file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pprint 优美的输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re 正则表达式\n",
    "\n",
    "[Python正则表达式指南](http://www.cnblogs.com/huxi/archive/2010/07/04/1771073.html)  \n",
    "[官方re](https://docs.python.org/2/library/re.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#设置一种匹配模式，再去匹配\n",
    "prog = re.compile(pattern)\n",
    "result = prog.match(string)\n",
    "\n",
    "#是否可匹配\n",
    "if prog.match(string)\n",
    "\n",
    "#直接用一种模式匹配，一次性\n",
    "result = re.match(pattern, string)\n",
    "\n",
    "#group 用法，默认参数为0\n",
    "m = re.match(r\"(\\w+) (\\w+)\", \"Isaac Newton, physicist\")\n",
    "m.group(0)       # The entire match\n",
    "#'Isaac Newton'\n",
    "m.group(1)       # The first parenthesized subgroup.\n",
    "#'Isaac'\n",
    "m.group(2)       # The second parenthesized subgroup.\n",
    "#'Newton'\n",
    "m.group(1, 2)    # Multiple arguments give us a tuple.\n",
    "#('Isaac', 'Newton')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### request module\n",
    "1. get和post区别  \n",
    "get method：url中带参数  \n",
    "post method：url中不带参数  \n",
    "\n",
    "2. post的data参数查看方法：打开网页检查->network->在网页中提交查询->Name中第一个->Headers->Form Data\n",
    "3. 保留会话\n",
    "s = requests.Session()  \n",
    "r = s.get()  \n",
    "p = s.post()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practice For Scraping\n",
    "1. look at how a browser makes requests\n",
    "2. emulate in code\n",
    "3. if stuff blows up .look at your http traffic\n",
    "4. return to 1 until it work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defaultdict \n",
    "设置默认初始的value值，比如int ，则是0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 4), ('p', 2), ('s', 4), ('m', 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "s = 'mississippi'\n",
    "d = defaultdict(int)\n",
    "for k in s:\n",
    "     d[k] += 1\n",
    "d.items()\n",
    "#[('i', 4), ('p', 2), ('s', 4), ('m', 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SET用法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boy', 'gril', 'w'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = set(['boy'])\n",
    "a.add('gril')\n",
    "a.add('w')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b', 'gril', 'o', 'w', 'y'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = set('boy')\n",
    "a.add('gril')\n",
    "a.add('w')\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
