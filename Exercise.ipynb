{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read csv file\n",
    "Your task is to read the input DATAFILE line by line, and for the first 10 lines (not including the header) split each line on \",\" and then for each line, create a dictionary. where the key is the header title of the field, and the value is the value of that field in the row. The function parse_file should return a list of dictionaries, each data line in the file being a single list entry. Field names and values should not contain extra whitespace, like spaces or newline characters. You can use the Python string method strip() to remove the extra whitespace. You have to parse only the first 10 data lines in this exercise, so the returned list should have 10 entries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def parse_file(datafile):\n",
    "    data = []\n",
    "    \n",
    "    with open(datafile, \"r\") as f:\n",
    "        header = f.readline().split(',')\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            if count == 10:\n",
    "                break\n",
    "            fields = line.split(',')\n",
    "            entry = {}\n",
    "            for i,value in enumerate(fields):\n",
    "                entry[header[i].strip()] = value.strip()\n",
    "            data.append(entry)\n",
    "            count = count+1\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(\"/Users/weidian1/Desktop/data.csv\",\"r\") as f:\n",
    "    step = []\n",
    "    accuracy = []\n",
    "    for index,line in enumerate(f):\n",
    "        value = line.split(\":\")[1]\n",
    "        if index%2 == 0:\n",
    "            step.append(float(value))\n",
    "        else:\n",
    "            accuracy.append(float(value[1:5])/100.)\n",
    "with open('output.csv', 'wb') as csvfile:\n",
    "    w = csv.writer(csvfile, delimiter=',')\n",
    "    for i in range(len(step)):\n",
    "        w.writerow([step[i] ,accuracy[i]])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pprint  #pprint.pprint  优美的输出\n",
    "#import os\n",
    "def parse_csv(datafile):\n",
    "    data = []\n",
    "    count = 0\n",
    "    with open(datafile ,'rb') as sd:\n",
    "        r = csv.DictReader(sd)\n",
    "        for line in r :\n",
    "            if count == 10:\n",
    "                break\n",
    "            data.append(line)\n",
    "            count = count+1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATADIR = \"/Users/weidian1/Documents/Study/nanodegreee/P3\"\n",
    "DATAFILE = \"beatles-diskography.csv\"\n",
    "\n",
    "def test(DATADIR,DATAFILE,parse_f):\n",
    "    datafile = os.path.join(DATADIR, DATAFILE)\n",
    "    d = parse_f(datafile)\n",
    "    firstline = {'Title': 'Please Please Me', 'UK Chart Position': '1', 'Label': 'Parlophone(UK)', 'Released': '22 March 1963', 'US Chart Position': '-', 'RIAA Certification': 'Platinum', 'BPI Certification': 'Gold'}\n",
    "    tenthline = {'Title': '', 'UK Chart Position': '1', 'Label': 'Parlophone(UK)', 'Released': '10 July 1964', 'US Chart Position': '-', 'RIAA Certification': '', 'BPI Certification': 'Gold'}\n",
    "    #print d[0]\n",
    "    assert d[0] == firstline\n",
    "    assert d[9] == tenthline\n",
    "\n",
    "    \n",
    "test(DATADIR,DATAFILE,parse_f=parse_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('eggs.csv', 'wb') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter=',')\n",
    "                            #,quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    spamwriter.writerow(['Spam'] * 5 + ['Baked Beans'])\n",
    "    spamwriter.writerow(['Spam', 'Lovely Spam', 'Wonderful Spam'])\n",
    "    spamwriter.writerow(['FAR_WEST']+ ['Max Load','Year'])\n",
    "    \n",
    "#exapmle outfile\n",
    "#Spam,Spam,Spam,Spam,Spam,Baked Beans\n",
    "#Spam,Lovely Spam,Wonderful Spam\n",
    "#FAR_WEST,Max Load,Year \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read xls file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List Comprehension\n",
      "data[3][2]: 1036.088697\n",
      "\n",
      "Cells in a nested loop:\n",
      "41277.0833333 9238.73731 1438.20528 1565.442856 916.708348 14010.903488 3027.98334 6165.211119 1157.741663 37520.933404 \n",
      "ROWS, COLUMNS, and CELLS:\n",
      "Number of rows in the sheet: 7296\n",
      "Type of data in cell (row 3, col 2): 2\n",
      "Value in cell (row 3, col 2): 1036.088697\n",
      "Get a slice of values in column 3, from rows 1-3:\n",
      "[1411.7505669999982, 1403.4722870000019, 1395.053150000001]\n",
      "\n",
      "DATES:\n",
      "Type of data in cell (row 1, col 0): 3\n",
      "Time in Excel format: 41275.0416667\n",
      "Convert time to a Python datetime tuple, from the Excel float: (2013, 1, 1, 1, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "#pip install xlrd\n",
    "import xlrd\n",
    "datedir = \"/Users/weidian1/Documents/Study/nanodegreee/P3\"\n",
    "datafile = \"2013_ERCOT_Hourly_Load_Data.xls\"\n",
    "\n",
    "\n",
    "def parse_file(datedir,datafile):\n",
    "    data = os.path.join(datedir, datafile)\n",
    "    workbook = xlrd.open_workbook(data)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "\n",
    "    data = [[sheet.cell_value(r, col) \n",
    "                for col in range(sheet.ncols)] \n",
    "                    for r in range(sheet.nrows)]\n",
    "\n",
    "    print \"\\nList Comprehension\"\n",
    "    print \"data[3][2]:\",\n",
    "    print data[3][2]\n",
    "\n",
    "    print \"\\nCells in a nested loop:\"    \n",
    "    for row in range(sheet.nrows):\n",
    "        for col in range(sheet.ncols):\n",
    "            if row == 50:\n",
    "                print sheet.cell_value(row, col),\n",
    "\n",
    "\n",
    "    ### other useful methods:\n",
    "    print \"\\nROWS, COLUMNS, and CELLS:\"\n",
    "    print \"Number of rows in the sheet:\", \n",
    "    print sheet.nrows\n",
    "    print \"Type of data in cell (row 3, col 2):\", \n",
    "    print sheet.cell_type(3, 2)\n",
    "    print \"Value in cell (row 3, col 2):\", \n",
    "    print sheet.cell_value(3, 2)\n",
    "    print \"Get a slice of values in column 3, from rows 1-3:\"\n",
    "    print sheet.col_values(3, start_rowx=1, end_rowx=4)\n",
    "\n",
    "    print \"\\nDATES:\"\n",
    "    print \"Type of data in cell (row 1, col 0):\", \n",
    "    print sheet.cell_type(1, 0)\n",
    "    exceltime = sheet.cell_value(1, 0)\n",
    "    print \"Time in Excel format:\",\n",
    "    print exceltime\n",
    "    print \"Convert time to a Python datetime tuple, from the Excel float:\",\n",
    "    print xlrd.xldate_as_tuple(exceltime, 0)\n",
    "\n",
    "    return data\n",
    "\n",
    "data = parse_file(datedir,datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "Find the time and value of max load for each of the regions\n",
    "COAST, EAST, FAR_WEST, NORTH, NORTH_C, SOUTHERN, SOUTH_C, WEST\n",
    "and write the result out in a csv file, using pipe character | as the delimiter.\n",
    "\n",
    "An example output can be seen in the \"example.csv\" file.\n",
    "'''\n",
    "\n",
    "import xlrd\n",
    "import os\n",
    "import csv\n",
    "from zipfile import ZipFile\n",
    "datadir = \"/Users/weidian1/Documents/Study/nanodegreee/P3\"\n",
    "datafile = \"2013_ERCOT_Hourly_Load_Data.xls\"\n",
    "outfile = \"2013_Max_Loads.csv\"\n",
    "\n",
    "\n",
    "def open_zip(datadir,datafile):\n",
    "    data = os.path.join(datadir, datafile)\n",
    "    with ZipFile('{0}.zip'.format(data), 'r') as myzip:\n",
    "        myzip.extractall()\n",
    "\n",
    "\n",
    "def parse_file(datadir,datafile):\n",
    "    datafile = os.path.join(datadir, datafile)\n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "    data = {}\n",
    "    # process all rows that contain station data\n",
    "    for n in range (1, 9):\n",
    "        station = sheet.cell_value(0, n)\n",
    "        cv = sheet.col_values(n, start_rowx=1, end_rowx=None)\n",
    "\n",
    "        maxval = max(cv)\n",
    "        maxpos = cv.index(maxval) + 1\n",
    "        maxtime = sheet.cell_value(maxpos, 0)\n",
    "        realtime = xlrd.xldate_as_tuple(maxtime, 0)\n",
    "        data[station] = {\"maxval\": maxval,\n",
    "                         \"maxtime\": realtime}\n",
    "    return data\n",
    "\n",
    "def save_file(data, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        w = csv.writer(f, delimiter='|')\n",
    "        w.writerow([\"Station\", \"Year\", \"Month\", \"Day\", \"Hour\", \"Max Load\"])\n",
    "        for s in data:\n",
    "            year, month, day, hour, _ , _= data[s][\"maxtime\"]\n",
    "            w.writerow([s, year, month, day, hour, data[s][\"maxval\"]])\n",
    "    \n",
    "def test():\n",
    "#    open_zip(datedir,datafile)\n",
    "    data = parse_file(datadir,datafile)\n",
    "    save_file(data, outfile)\n",
    "\n",
    "    number_of_rows = 0\n",
    "    stations = []\n",
    "\n",
    "    ans = {'FAR_WEST': {'Max Load': '2281.2722140000024',\n",
    "                        'Year': '2013',\n",
    "                        'Month': '6',\n",
    "                        'Day': '26',\n",
    "                        'Hour': '17'}}\n",
    "    correct_stations = ['COAST', 'EAST', 'FAR_WEST', 'NORTH',\n",
    "                        'NORTH_C', 'SOUTHERN', 'SOUTH_C', 'WEST']\n",
    "    fields = ['Year', 'Month', 'Day', 'Hour', 'Max Load']\n",
    "\n",
    "    with open(outfile) as of:\n",
    "        csvfile = csv.DictReader(of, delimiter=\"|\")\n",
    "        for line in csvfile:\n",
    "            station = line['Station']\n",
    "            if station == 'FAR_WEST':\n",
    "                for field in fields:\n",
    "                    # Check if 'Max Load' is within .1 of answer\n",
    "                    if field == 'Max Load':\n",
    "                        max_answer = round(float(ans[station][field]), 1)\n",
    "                        max_line = round(float(line[field]), 1)\n",
    "                        assert max_answer == max_line\n",
    "\n",
    "                    # Otherwise check for equality\n",
    "                    else:\n",
    "                        assert ans[station][field] == line[field]\n",
    "\n",
    "            number_of_rows += 1\n",
    "            stations.append(station)\n",
    "\n",
    "        # Output should be 8 lines not including header\n",
    "        assert number_of_rows == 8\n",
    "\n",
    "        # Check Station Names\n",
    "        assert set(stations) == set(correct_stations)\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get JSON fomart data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requesting http://musicbrainz.org/ws/2/artist/?query=artist%3ANirvana&fmt=json\n",
      "{\n",
      "    \"artists\": [\n",
      "        {\n",
      "            \"aliases\": [\n",
      "                {\n",
      "                    \"begin-date\": null, \n",
      "                    \"end-date\": null, \n",
      "                    \"locale\": null, \n",
      "                    \"name\": \"Nirvana US\", \n",
      "                    \"primary\": null, \n",
      "                    \"sort-name\": \"Nirvana US\", \n",
      "                    \"type\": null\n",
      "                }\n",
      "            ], \n",
      "            \"area\": {\n",
      "                \"id\": \"489ce91b-6658-3307-9877-795b68554c98\", \n",
      "                \"name\": \"United States\", \n",
      "                \"sort-name\": \"United States\"\n",
      "            }, \n",
      "            \"begin-area\": {\n",
      "                \"id\": \"a640b45c-c173-49b1-8030-973603e895b5\", \n",
      "                \"name\": \"Aberdeen\", \n",
      "                \"sort-name\": \"Aberdeen\"\n",
      "            }, \n",
      "            \"country\": \"US\", \n",
      "            \"disambiguation\": \"90s US grunge band\", \n",
      "            \"id\": \"5b11f4ce-a62d-471e-81fc-a69a8278c7da\", \n",
      "            \"life-span\": {\n",
      "                \"begin\": \"1988-01\", \n",
      "                \"end\": \"1994-04-05\", \n",
      "                \"ended\": true\n",
      "            }, \n",
      "            \"name\": \"Nirvana\", \n",
      "            \"score\": \"100\", \n",
      "            \"sort-name\": \"Nirvana\", \n",
      "            \"tags\": [\n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"punk\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"legendary\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"90\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"seattle\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"northwest\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"alternative\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"rock and indie\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"usa\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"am\\u00e9ricain\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"united states\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"kurt cobain\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"90s\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 4, \n",
      "                    \"name\": \"alternative rock\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"band\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 13, \n",
      "                    \"name\": \"grunge\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 9, \n",
      "                    \"name\": \"rock\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"acoustic rock\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"noise rock\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"nirvana\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 5, \n",
      "                    \"name\": \"american\"\n",
      "                }\n",
      "            ], \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"8a754a16-0027-3a29-b6d7-2b40ea0481ed\", \n",
      "                \"name\": \"United Kingdom\", \n",
      "                \"sort-name\": \"United Kingdom\"\n",
      "            }, \n",
      "            \"begin-area\": {\n",
      "                \"id\": \"f03d09b3-39dc-4083-afd6-159e3f0d462f\", \n",
      "                \"name\": \"London\", \n",
      "                \"sort-name\": \"London\"\n",
      "            }, \n",
      "            \"country\": \"GB\", \n",
      "            \"disambiguation\": \"60s band from the UK\", \n",
      "            \"id\": \"9282c8b4-ca0b-4c6b-b7e3-4f7762dfc4d6\", \n",
      "            \"life-span\": {\n",
      "                \"begin\": \"1967\", \n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Nirvana\", \n",
      "            \"score\": \"100\", \n",
      "            \"sort-name\": \"Nirvana\", \n",
      "            \"tags\": [\n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"rock\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"pop\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"progressive rock\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"orchestral\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"british\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"power pop\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"psychedelic rock\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"soft rock\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"symphonic rock\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"english\"\n",
      "                }\n",
      "            ], \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"6a264f94-6ff1-30b1-9a81-41f7bfabd616\", \n",
      "                \"name\": \"Finland\", \n",
      "                \"sort-name\": \"Finland\"\n",
      "            }, \n",
      "            \"country\": \"FI\", \n",
      "            \"disambiguation\": \"Early 1980's Finnish punk band\", \n",
      "            \"id\": \"85af0709-95db-4fbc-801a-120e9f4766d0\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Nirvana\", \n",
      "            \"score\": \"100\", \n",
      "            \"sort-name\": \"Nirvana\", \n",
      "            \"tags\": [\n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"punk\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"finland\"\n",
      "                }\n",
      "            ], \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"disambiguation\": \"French band from Martigues, activ during the 70s.\", \n",
      "            \"id\": \"c49d69dc-e008-47cf-b5ff-160fafb1fe1f\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Nirvana\", \n",
      "            \"score\": \"100\", \n",
      "            \"sort-name\": \"Nirvana\"\n",
      "        }, \n",
      "        {\n",
      "            \"disambiguation\": \"founded in 1987 by a Michael Jackson double/imitator\", \n",
      "            \"id\": \"3aa878c0-224b-41e5-abd1-63be359d2bca\", \n",
      "            \"life-span\": {\n",
      "                \"begin\": \"1987\", \n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Nirvana\", \n",
      "            \"score\": \"100\", \n",
      "            \"sort-name\": \"Nirvana\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"b305320e-c158-43f4-b5be-4450e2f99a32\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"El Nirvana\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Nirvana, El\"\n",
      "        }, \n",
      "        {\n",
      "            \"aliases\": [\n",
      "                {\n",
      "                    \"begin-date\": null, \n",
      "                    \"end-date\": null, \n",
      "                    \"locale\": null, \n",
      "                    \"name\": \"Nirvana\", \n",
      "                    \"primary\": null, \n",
      "                    \"sort-name\": \"Nirvana\", \n",
      "                    \"type\": null\n",
      "                }, \n",
      "                {\n",
      "                    \"begin-date\": null, \n",
      "                    \"end-date\": null, \n",
      "                    \"locale\": null, \n",
      "                    \"name\": \"Prophet 2002\", \n",
      "                    \"primary\": null, \n",
      "                    \"sort-name\": \"Prophet 2002\", \n",
      "                    \"type\": null\n",
      "                }\n",
      "            ], \n",
      "            \"area\": {\n",
      "                \"id\": \"23d10872-f5ae-3f0c-bf55-332788a16ecb\", \n",
      "                \"name\": \"Sweden\", \n",
      "                \"sort-name\": \"Sweden\"\n",
      "            }, \n",
      "            \"country\": \"SE\", \n",
      "            \"disambiguation\": \"Swedish death metal band\", \n",
      "            \"id\": \"f2dfdff9-3862-4be0-bf85-9c833fa3059e\", \n",
      "            \"life-span\": {\n",
      "                \"begin\": \"1988\", \n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Nirvana 2002\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Nirvana 2002\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"329c04ae-3b73-4ca3-996f-75608ab1befb\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Nirvana Singh\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Singh, Nirvana\", \n",
      "            \"type\": \"Person\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"489ce91b-6658-3307-9877-795b68554c98\", \n",
      "                \"name\": \"United States\", \n",
      "                \"sort-name\": \"United States\"\n",
      "            }, \n",
      "            \"country\": \"US\", \n",
      "            \"id\": \"c3a64a25-251b-4d03-afba-1471440245b8\", \n",
      "            \"life-span\": {\n",
      "                \"begin\": \"2009\", \n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Approaching Nirvana\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Approaching Nirvana\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"489ce91b-6658-3307-9877-795b68554c98\", \n",
      "                \"name\": \"United States\", \n",
      "                \"sort-name\": \"United States\"\n",
      "            }, \n",
      "            \"country\": \"US\", \n",
      "            \"gender\": \"female\", \n",
      "            \"id\": \"206419e0-3a7a-49ce-8437-4e757767d02b\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Nirvana Savoury\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Savoury, Nirvana\", \n",
      "            \"type\": \"Person\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"86f9ae24-ba2a-4d55-9275-0b89b85f6e3a\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Weed Nirvana\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Weed Nirvana\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"e8ad73e9-9e7f-41c4-a395-6e29260ff1df\", \n",
      "                \"name\": \"Graz\", \n",
      "                \"sort-name\": \"Graz\"\n",
      "            }, \n",
      "            \"begin-area\": {\n",
      "                \"id\": \"e8ad73e9-9e7f-41c4-a395-6e29260ff1df\", \n",
      "                \"name\": \"Graz\", \n",
      "                \"sort-name\": \"Graz\"\n",
      "            }, \n",
      "            \"disambiguation\": \"Nirvana-Coverband\", \n",
      "            \"id\": \"46d8dae4-abec-438b-9c62-a3dbb2aaa1b7\", \n",
      "            \"life-span\": {\n",
      "                \"begin\": \"2000\", \n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Nirvana Teen Spirit\", \n",
      "            \"score\": \"50\", \n",
      "            \"sort-name\": \"Nirvana Teen Spirit\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"c621114d-73cc-4832-8afe-f13dc261e5af\", \n",
      "                \"name\": \"Gatineau\", \n",
      "                \"sort-name\": \"Gatineau\"\n",
      "            }, \n",
      "            \"begin-area\": {\n",
      "                \"id\": \"c621114d-73cc-4832-8afe-f13dc261e5af\", \n",
      "                \"name\": \"Gatineau\", \n",
      "                \"sort-name\": \"Gatineau\"\n",
      "            }, \n",
      "            \"id\": \"02c4e6bb-7b7a-4686-8c23-df01bfd42b0e\", \n",
      "            \"life-span\": {\n",
      "                \"begin\": \"2012-04-05\", \n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Sappy Nirvana Tribute\", \n",
      "            \"score\": \"50\", \n",
      "            \"sort-name\": \"Sappy Nirvana Tribute\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"e1388435-f80d-434a-9980-f1c9f5aa9b90\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Nirvana Sitar & String Group\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"Nirvana Sitar & String Group\"\n",
      "        }\n",
      "    ], \n",
      "    \"count\": 14, \n",
      "    \"created\": \"2016-09-22T03:46:46.648Z\", \n",
      "    \"offset\": 0\n",
      "}\n",
      "\n",
      "ARTIST:\n",
      "{\n",
      "    \"area\": {\n",
      "        \"id\": \"8a754a16-0027-3a29-b6d7-2b40ea0481ed\", \n",
      "        \"name\": \"United Kingdom\", \n",
      "        \"sort-name\": \"United Kingdom\"\n",
      "    }, \n",
      "    \"begin-area\": {\n",
      "        \"id\": \"f03d09b3-39dc-4083-afd6-159e3f0d462f\", \n",
      "        \"name\": \"London\", \n",
      "        \"sort-name\": \"London\"\n",
      "    }, \n",
      "    \"country\": \"GB\", \n",
      "    \"disambiguation\": \"60s band from the UK\", \n",
      "    \"id\": \"9282c8b4-ca0b-4c6b-b7e3-4f7762dfc4d6\", \n",
      "    \"life-span\": {\n",
      "        \"begin\": \"1967\", \n",
      "        \"ended\": null\n",
      "    }, \n",
      "    \"name\": \"Nirvana\", \n",
      "    \"score\": \"100\", \n",
      "    \"sort-name\": \"Nirvana\", \n",
      "    \"tags\": [\n",
      "        {\n",
      "            \"count\": 1, \n",
      "            \"name\": \"rock\"\n",
      "        }, \n",
      "        {\n",
      "            \"count\": 1, \n",
      "            \"name\": \"pop\"\n",
      "        }, \n",
      "        {\n",
      "            \"count\": 1, \n",
      "            \"name\": \"progressive rock\"\n",
      "        }, \n",
      "        {\n",
      "            \"count\": 1, \n",
      "            \"name\": \"orchestral\"\n",
      "        }, \n",
      "        {\n",
      "            \"count\": 1, \n",
      "            \"name\": \"british\"\n",
      "        }, \n",
      "        {\n",
      "            \"count\": 1, \n",
      "            \"name\": \"power pop\"\n",
      "        }, \n",
      "        {\n",
      "            \"count\": 1, \n",
      "            \"name\": \"psychedelic rock\"\n",
      "        }, \n",
      "        {\n",
      "            \"count\": 1, \n",
      "            \"name\": \"soft rock\"\n",
      "        }, \n",
      "        {\n",
      "            \"count\": 1, \n",
      "            \"name\": \"symphonic rock\"\n",
      "        }, \n",
      "        {\n",
      "            \"count\": 1, \n",
      "            \"name\": \"english\"\n",
      "        }\n",
      "    ], \n",
      "    \"type\": \"Group\"\n",
      "}\n",
      "requesting http://musicbrainz.org/ws/2/artist/9282c8b4-ca0b-4c6b-b7e3-4f7762dfc4d6?fmt=json&inc=releases\n",
      "\n",
      "ONE RELEASE:\n",
      "{\n",
      "  \"barcode\": null, \n",
      "  \"country\": \"GB\", \n",
      "  \"date\": \"1969\", \n",
      "  \"disambiguation\": \"\", \n",
      "  \"id\": \"0b44cb36-550a-491d-bfd9-8751271f9de7\", \n",
      "  \"packaging\": null, \n",
      "  \"packaging-id\": null, \n",
      "  \"quality\": \"normal\", \n",
      "  \"release-events\": [\n",
      "    {\n",
      "      \"area\": {\n",
      "        \"disambiguation\": \"\", \n",
      "        \"id\": \"8a754a16-0027-3a29-b6d7-2b40ea0481ed\", \n",
      "        \"iso-3166-1-codes\": [\n",
      "          \"GB\"\n",
      "        ], \n",
      "        \"name\": \"United Kingdom\", \n",
      "        \"sort-name\": \"United Kingdom\"\n",
      "      }, \n",
      "      \"date\": \"1969\"\n",
      "    }\n",
      "  ], \n",
      "  \"status\": \"Official\", \n",
      "  \"status-id\": \"4e304316-386d-3409-af2e-78857eec5cfe\", \n",
      "  \"text-representation\": {\n",
      "    \"language\": \"eng\", \n",
      "    \"script\": \"Latn\"\n",
      "  }, \n",
      "  \"title\": \"To Markos III\"\n",
      "}\n",
      "\n",
      "ALL TITLES:\n",
      "To Markos III\n",
      "Travelling on a Cloud\n",
      "Songs Of Love And Praise\n",
      "Songs of Love and Praise\n",
      "Songs of Love and Praise\n",
      "Secret Theatre\n",
      "The Story of Simon Simopath\n",
      "Me And My Friend\n",
      "All of Us\n",
      "The Story of Simon Simopath\n",
      "To Markos III\n",
      "Chemistry\n",
      "Local Anaesthetic\n",
      "Orange & Blue\n",
      "Pentecost Hotel\n",
      "Black Flower\n",
      "All of Us\n"
     ]
    }
   ],
   "source": [
    "# To experiment with this code freely you will have to run this code locally.\n",
    "# Take a look at the main() function for an example of how to use the code.\n",
    "# We have provided example json output in the other code editor tabs for you to\n",
    "# look at, but you will not be able to run any queries through our UI.\n",
    "import json\n",
    "import requests\n",
    "\n",
    "\n",
    "BASE_URL = \"http://musicbrainz.org/ws/2/\"\n",
    "ARTIST_URL = BASE_URL + \"artist/\"\n",
    "\n",
    "# query parameters are given to the requests.get function as a dictionary; this\n",
    "# variable contains some starter parameters.\n",
    "query_type = {  \"simple\": {},\n",
    "                \"atr\": {\"inc\": \"aliases+tags+ratings\"},\n",
    "                \"aliases\": {\"inc\": \"aliases\"},\n",
    "                \"releases\": {\"inc\": \"releases\"}}\n",
    "\n",
    "\n",
    "def query_site(url, params, uid=\"\", fmt=\"json\"):\n",
    "    # This is the main function for making queries to the musicbrainz API.\n",
    "    # A json document should be returned by the query.\n",
    "    params[\"fmt\"] = fmt\n",
    "    r = requests.get(url + uid, params=params)\n",
    "    print \"requesting\", r.url\n",
    "\n",
    "    #检测响应状态码\n",
    "    if r.status_code == requests.codes.ok:  \n",
    "        return r.json()\n",
    "    else:\n",
    "        r.raise_for_status()\n",
    "\n",
    "\n",
    "def query_by_name(url, params, name):\n",
    "    # This adds an artist name to the query parameters before making\n",
    "    # an API call to the function above.\n",
    "    params[\"query\"] = \"artist:\" + name\n",
    "    return query_site(url, params)\n",
    "\n",
    "\n",
    "def pretty_print(data, indent=4):\n",
    "    # After we get our output, we can format it to be more readable\n",
    "    # by using this function.\n",
    "    if type(data) == dict:\n",
    "        print json.dumps(data, indent=indent, sort_keys=True)\n",
    "    else:\n",
    "        print data\n",
    "\n",
    "\n",
    "def main():\n",
    "    '''\n",
    "    Modify the function calls and indexing below to answer the questions on\n",
    "    the next quiz. HINT: Note how the output we get from the site is a\n",
    "    multi-level JSON document, so try making print statements to step through\n",
    "    the structure one level at a time or copy the output to a separate output\n",
    "    file.\n",
    "    '''\n",
    "    results = query_by_name(ARTIST_URL, query_type[\"simple\"], \"Nirvana\")\n",
    "    pretty_print(results)\n",
    "\n",
    "    artist_id = results[\"artists\"][1][\"id\"]\n",
    "    print \"\\nARTIST:\"\n",
    "    pretty_print(results[\"artists\"][1])\n",
    "\n",
    "    artist_data = query_site(ARTIST_URL, query_type[\"releases\"], artist_id)\n",
    "    releases = artist_data[\"releases\"]\n",
    "    print \"\\nONE RELEASE:\"\n",
    "    pretty_print(releases[0], indent=2)\n",
    "    release_titles = [r[\"title\"] for r in releases]\n",
    "\n",
    "    print \"\\nALL TITLES:\"\n",
    "    for t in release_titles:\n",
    "        print t\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requesting http://musicbrainz.org/ws/2/artist/?query=artist%3ABEATLES&fmt=json\n",
      "requesting http://musicbrainz.org/ws/2/artist/bc569a61-dd62-4758-86c6-e99dcb1fdda6?fmt=json&inc=aliases\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "\n",
    "BASE_URL = \"http://musicbrainz.org/ws/2/\"\n",
    "ARTIST_URL = BASE_URL + \"artist/\"\n",
    "\n",
    "# query parameters are given to the requests.get function as a dictionary; this\n",
    "# variable contains some starter parameters.\n",
    "query_type = {  \"simple\": {},\n",
    "                \"atr\": {\"inc\": \"aliases+tags+ratings\"},\n",
    "                \"aliases\": {\"inc\": \"aliases\"},\n",
    "                \"releases\": {\"inc\": \"releases\"}}\n",
    "\n",
    "\n",
    "def query_site(url, params, uid=\"\", fmt=\"json\"):\n",
    "    # This is the main function for making queries to the musicbrainz API.\n",
    "    # A json document should be returned by the query.\n",
    "    params[\"fmt\"] = fmt\n",
    "    r = requests.get(url + uid, params=params)\n",
    "    print \"requesting\", r.url\n",
    "\n",
    "    #检测响应状态码\n",
    "    if r.status_code == requests.codes.ok:  \n",
    "        return r.json()\n",
    "    else:\n",
    "        r.raise_for_status()\n",
    "\n",
    "\n",
    "def query_by_name(url, params, name):\n",
    "    # This adds an artist name to the query parameters before making\n",
    "    # an API call to the function above.\n",
    "    params[\"query\"] = \"artist:\" + name\n",
    "    return query_site(url, params)\n",
    "\n",
    "\n",
    "def pretty_print(data, indent=4):\n",
    "    # After we get our output, we can format it to be more readable\n",
    "    # by using this function.\n",
    "    if type(data) == dict:\n",
    "        print json.dumps(data, indent=indent, sort_keys=True)\n",
    "    else:\n",
    "        print data\n",
    "\n",
    "\n",
    "def main():\n",
    "    '''\n",
    "    Modify the function calls and indexing below to answer the questions on\n",
    "    the next quiz. HINT: Note how the output we get from the site is a\n",
    "    multi-level JSON document, so try making print statements to step through\n",
    "    the structure one level at a time or copy the output to a separate output\n",
    "    file.\n",
    "    '''\n",
    "    results = query_by_name(ARTIST_URL, query_type[\"simple\"], \"BEATLES\")\n",
    "#    pretty_print(results)\n",
    "\n",
    "    artist_id = results[\"artists\"][1][\"id\"]\n",
    "#    print \"\\nARTIST:\"\n",
    "#    pretty_print(results[\"artists\"][1])\n",
    "\n",
    "    artist_data = query_site(ARTIST_URL, query_type[\"aliases\"], artist_id)\n",
    "    releases = artist_data[\"aliases\"]\n",
    "#    print \"\\nONE RELEASE:\"\n",
    "    print releases\n",
    "#    pretty_print(releases[0], indent=2)\n",
    "#    release_titles = [r[\"title\"] for r in releases]\n",
    "\n",
    "#    print \"\\nALL TITLES:\"\n",
    "#    for t in release_titles:\n",
    "#        print t\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='api.nytimes.com', port=80): Max retries exceeded with url: /svc/mostpopular/v2/mostviewed/all-sections/1.json?api-key=d47c4bf33872431f80cb97e2e8a540d3&offset=0 (Caused by NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at 0x105217f50>: Failed to establish a new connection: [Errno 60] Operation timed out',))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-72c8552a86ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0msave_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'viewed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-42-72c8552a86ff>\u001b[0m in \u001b[0;36msave_file\u001b[0;34m(kind, period)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# This will process all results, by calling the API repeatedly with supplied offset value,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;31m# combine the data and then write all results in a file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_popular\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mURL_POPULAR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"viewed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0mnum_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_results\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mfull_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-72c8552a86ff>\u001b[0m in \u001b[0;36mget_popular\u001b[0;34m(url, kind, days, section, offset)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"most{0}/{1}/{2}.json\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_site\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"popular\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-72c8552a86ff>\u001b[0m in \u001b[0;36mquery_site\u001b[0;34m(url, target, offset)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"api-key\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAPI_KEY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"offset\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/weidian1/anaconda/lib/python2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/weidian1/anaconda/lib/python2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/weidian1/anaconda/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    466\u001b[0m         }\n\u001b[1;32m    467\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/weidian1/anaconda/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/weidian1/anaconda/lib/python2.7/site-packages/requests/adapters.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    435\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='api.nytimes.com', port=80): Max retries exceeded with url: /svc/mostpopular/v2/mostviewed/all-sections/1.json?api-key=d47c4bf33872431f80cb97e2e8a540d3&offset=0 (Caused by NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at 0x105217f50>: Failed to establish a new connection: [Errno 60] Operation timed out',))"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This exercise shows some important concepts that you should be aware about:\n",
    "- using codecs module to write unicode files\n",
    "- using authentication with web APIs\n",
    "- using offset when accessing web APIs\n",
    "\n",
    "To run this code locally you have to register at the NYTimes developer site \n",
    "and get your own API key. You will be able to complete this exercise in our UI\n",
    "without doing so, as we have provided a sample result.\n",
    "\n",
    "Your task is to process the saved file that represents the most popular\n",
    "articles (by view count) from the last day, and return the following data:\n",
    "- list of dictionaries, where the dictionary key is \"section\" and value is \"title\"\n",
    "- list of URLs for all media entries with \"format\": \"Standard Thumbnail\"\n",
    "\n",
    "All your changes should be in the article_overview function.\n",
    "The rest of functions are provided for your convenience, if you want to access\n",
    "the API by yourself.\n",
    "\"\"\"\n",
    "import json\n",
    "import codecs\n",
    "import requests\n",
    "\n",
    "URL_MAIN = \"http://api.nytimes.com/svc/\"\n",
    "URL_POPULAR = URL_MAIN + \"mostpopular/v2/\"\n",
    "API_KEY = { \"popular\": \"d47c4bf33872431f80cb97e2e8a540d3\",\n",
    "            \"article\": \"article\"}\n",
    "\n",
    "\n",
    "def get_from_file(kind, period):\n",
    "    filename = \"popular-{0}-{1}.json\".format(kind, period)\n",
    "    with open(filename, \"r\") as f:\n",
    "        return json.loads(f.read())\n",
    "\n",
    "\n",
    "def article_overview(kind, period):\n",
    "    data = get_from_file(kind, period)\n",
    "    titles = []\n",
    "    urls =[]\n",
    "    for asset in data:\n",
    "        titles.append({asset[\"section\"]: asset[\"title\"]})\n",
    "        for med in asset['media']:\n",
    "            for e in med['media-metadata']:\n",
    "                if e['format'] == \"Standard Thumbnail\":\n",
    "                     urls.append(e['url'])\n",
    "    return (titles, urls)\n",
    "\n",
    "def query_site(url, target, offset):\n",
    "    # This will set up the query with the API key and offset\n",
    "    # Web services often use offset paramter to return data in small chunks\n",
    "    # NYTimes returns 20 articles per request, if you want the next 20\n",
    "    # You have to provide the offset parameter\n",
    "    if API_KEY[\"popular\"] == \"\" or API_KEY[\"article\"] == \"\":\n",
    "        print \"You need to register for NYTimes Developer account to run this program.\"\n",
    "        print \"See Intructor notes for information\"\n",
    "        return False\n",
    "    params = {\"api-key\": API_KEY[target], \"offset\": offset}\n",
    "    r = requests.get(url, params = params)\n",
    "\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        return r.json()\n",
    "    else:\n",
    "        r.raise_for_status()\n",
    "\n",
    "\n",
    "def get_popular(url, kind, days, section=\"all-sections\", offset=0):\n",
    "    # This function will construct the query according to the requirements of the site\n",
    "    # and return the data, or print an error message if called incorrectly\n",
    "    if days not in [1,7,30]:\n",
    "        print \"Time period can be 1,7, 30 days only\"\n",
    "        return False\n",
    "    if kind not in [\"viewed\", \"shared\", \"emailed\"]:\n",
    "        print \"kind can be only one of viewed/shared/emailed\"\n",
    "        return False\n",
    "\n",
    "    url += \"most{0}/{1}/{2}.json\".format(kind, section, days)\n",
    "    data = query_site(url, \"popular\", offset)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_file(kind, period):\n",
    "    # This will process all results, by calling the API repeatedly with supplied offset value,\n",
    "    # combine the data and then write all results in a file.\n",
    "    data = get_popular(URL_POPULAR, \"viewed\", 1)\n",
    "    num_results = data[\"num_results\"]\n",
    "    full_data = []\n",
    "    with codecs.open(\"popular-{0}-{1}.json\".format(kind, period), encoding='utf-8', mode='w') as v:\n",
    "        for offset in range(0, num_results, 20):        \n",
    "            data = get_popular(URL_POPULAR, kind, period, offset=offset)\n",
    "            full_data += data[\"results\"]\n",
    "        \n",
    "        v.write(json.dumps(full_data, indent=2))\n",
    "\n",
    "\n",
    "def test():\n",
    "    titles, urls = article_overview(\"viewed\", 1)\n",
    "    assert len(titles) == 20\n",
    "    assert len(urls) == 30\n",
    "    assert titles[2] == {'Opinion': 'Professors, We Need You!'}\n",
    "    assert urls[20] == 'http://graphics8.nytimes.com/images/2014/02/17/sports/ICEDANCE/ICEDANCE-thumbStandard.jpg'\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    save_file(, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Here's your API Key for the Article Search API: d47c4bf33872431f80cb97e2e8a540d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65535\n"
     ]
    }
   ],
   "source": [
    "\"\"\"当python要做编码转换的时候，会借助于内部的编码，转换过程是这样的：\n",
    "原有编码 -> 内部编码 -> 目的编码\n",
    "python的内部是使用unicode来处理的，但是unicode的使用需要考虑的是它的编码格式有两种，一是UCS-2，它一共有65536个码位，另一种是UCS-4，它有2147483648g个码位。\"\"\"\n",
    "import sys\n",
    "print sys.maxunicode\n",
    "#如果输出的值为65535,那么就是UCS-2,如果输出是1114111就是UCS-4编码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "12 我爱北京\n",
      "12 我爱北京 <type 'unicode'>\n",
      "4 我爱北京 <type 'str'>\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "import codecs, sys\n",
    "\n",
    "print '-'*60\n",
    "# 创建gb2312编码器\n",
    "look  = codecs.lookup(\"gbk\")\n",
    "# 创建utf-8编码器\n",
    "look2 = codecs.lookup(\"utf-8\")\n",
    "\n",
    "a = \"我爱北京\"\n",
    "\n",
    "print len(a), a\n",
    "# 把a编码为内部的unicode, 但为什么方法名为decode呢，我的理解是把gbk的字符串解码为unicode\n",
    "b = look2.decode(a)\n",
    "# 返回的b[0]是数据，b[1]是长度，这个时候的类型是unicode了\n",
    "print b[1], b[0], type(b[0])\n",
    "# 把内部编码的unicode转换为gb2312编码的字符串，encode方法会返回一个字符串类型\n",
    "b2 = look2.encode(b[0])\n",
    "# 发现不一样的地方了吧？转换回来之后，字符串长度由12变为了4! 现在的返回的长度才是真正的字数，原来的是字节数\n",
    "print b2[1], b2[0], type(b2[0])\n",
    "# 虽然上面返回了字数，但并不意味着用len求b2[0]的长度就是4了，仍然还是12，仅仅是codecs.encode会统计字数\n",
    "print len(b2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API  \n",
    "[XPath_sytax](http://www.w3schools.com/xsl/xpath_syntax.asp)  \n",
    "[xml.etree.elementtree](https://docs.python.org/2/library/xml.etree.elementtree.html)\n",
    "[BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Title:\n",
      "Standardization of the functional syndesmosis widening by dynamic U.S examination\n",
      "\n",
      "            \n",
      "\n",
      "Author email addresses:\n",
      "omer@extremegate.com\n",
      "mcarmont@hotmail.com\n",
      "laver17@gmail.com\n",
      "nyska@internet-zahav.net\n",
      "kammarh@gmail.com\n",
      "gideon.mann.md@gmail.com\n",
      "barns.nz@gmail.com\n",
      "eukots@gmail.com\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pprint\n",
    "import codecs\n",
    "\n",
    "tree = ET.parse(codecs.open(\"exampleResearchArticle.xml\", encoding=\"UTF-8\"))\n",
    "root = tree.getroot()\n",
    "\n",
    "title = root.find('./fm/bibl/title')\n",
    "title_text = \"\"\n",
    "for p in title:\n",
    "    title_text += p.text\n",
    "print \"\\nTitle:\\n\",title_text\n",
    "\n",
    "print \"\\nAuthor email addresses:\"\n",
    "for a in root.findall('./fm/bibl/aug/au'):\n",
    "    email = a.find('email')\n",
    "    if email is not None:\n",
    "        print email.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# 搜索框中的选项  [{名：缩写},...]\n",
    "def options(soup, id):\n",
    "    option_values = []\n",
    "    carrier_list = soup.find(id = id)\n",
    "    #soup.find(\"a\", id=\"link3\")     tag='a',attribution id='link3'\n",
    "    for option in carrier_list.find_all('option'): #tag 是 option的所有element\n",
    "        option_values.append(option['value']) #响应的 value属性的值\n",
    "    return option_values\n",
    "\n",
    "#格式化输出 print(\"I'm %s. I'm %d year old\" % ('Vamei', 99))\n",
    "def print_list(label, codes):\n",
    "    print \"\\n%s:\"%label\n",
    "    for c in codes:\n",
    "        print c\n",
    "\n",
    "#提交表单， \n",
    "def post():        \n",
    "    #保留会话，get和post用一个会话\n",
    "    s = requests.Session()\n",
    "\n",
    "    r = s.get(\"http://www.transtats.bts.gov/Data_Elements.aspx?Data=2\")  \n",
    "    soup = BeautifulSoup(r.text)\n",
    "    viewstate = soup.find('input',id='__VIEWSTATE')['value']\n",
    "    eventvalidation = soup.find('input',id='__EVENTVALIDATION')['value']\n",
    "\n",
    "    r = s.post(\"http://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "                  data = {\"__EVENTTARGET\":\"\",\n",
    "                          \"__EVENTARGUMENT\":\"\",\n",
    "                          \"__VIEWSTATE\":viewstate,\n",
    "                          \"__EVENTVALIDATION\":eventvalidation,\n",
    "                          \"CarrierList\":CarrierList['Virgin America'],\n",
    "                          \"AirportList\":AirportList[' - Boston, MA: Logan International'],\n",
    "                          \"Submit\":'Submit'})\n",
    "def write_html(file_name):\n",
    "    with open(file_name,\"w\") as f:\n",
    "        f.write(r.text)\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # BeautifulSoup(markup, \"html.parser\")  parser library：‘lxml’，‘html.parser’ ...\n",
    "    soup = BeautifulSoup(open(\"virgin_and_logan_airport.html\"))\n",
    "    CarrierList = options(soup,'CarrierList')\n",
    "    #print_list(\"Carries\",codes)\n",
    "    CarrierList = options(soup,'AirportList')\n",
    "    #print_list(\"Airpirts\",codes)\n",
    "    post()\n",
    "    write_html(\"virgin_and_logan_airport_out.html\")\n",
    "    \n",
    "    \n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# \n",
    "import xml.etree.cElementTree as ET\n",
    "#dict subclass that calls a factory function to supply missing values。\n",
    "#defaultdict(default_factory), 按default_factory给key生成默认的value\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "osm_file = open(\"chicago.osm\", \"r\")\n",
    "\n",
    "#设置匹配模式：非空白字符，以1个或0个‘.’结束， 忽略大小写\n",
    "street_type_re = re.compile(r'\\S+\\.?$', re.IGNORECASE)\n",
    "street_types = defaultdict(int) #默认value=0\n",
    "\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "\n",
    "        street_types[street_type] += 1\n",
    "\n",
    "def print_sorted_dict(d):\n",
    "    keys = d.keys()\n",
    "    keys = sorted(keys, key=lambda s: s.lower())\n",
    "    for k in keys:\n",
    "        v = d[k]\n",
    "        print \"%s: %d\" % (k, v)\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.tag == \"tag\") and (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "def audit():\n",
    "    #ET.iterparse 不一次全部加载进内存，\n",
    "    for event, elem in ET.iterparse(osm_file):\n",
    "        if is_street_name(elem):\n",
    "            audit_street_type(street_types, elem.attrib['v'])\n",
    "    print_sorted_dict(street_types)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    audit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 4), ('p', 2), ('s', 4), ('m', 1)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "s = 'mississippi'\n",
    "d = defaultdict(int)\n",
    "for k in s:\n",
    "     d[k] += 1\n",
    "\n",
    "d.items()\n",
    "#[('i', 4), ('p', 2), ('s', 4), ('m', 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 4, 'm': 1, 'p': 2, 's': 4}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Blueprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "import xml.etree.cElementTree as ET\n",
    "#dict subclass that calls a factory function to supply missing values。\n",
    "#defaultdict(default_factory), 按default_factory给key生成默认的value\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "osm_file = open(\"chicago.osm\", \"r\")\n",
    "\n",
    "#设置匹配模式：非空白字符，以1个或0个‘.’结束， 忽略大小写\n",
    "street_type_re = re.compile(r'\\S+\\.?$', re.IGNORECASE)\n",
    "street_types = defaultdict(int) #默认value=0\n",
    "\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "\n",
    "        street_types[street_type] += 1\n",
    "\n",
    "def print_sorted_dict(d):\n",
    "    keys = d.keys()\n",
    "    keys = sorted(keys, key=lambda s: s.lower())\n",
    "    for k in keys:\n",
    "        v = d[k]\n",
    "        print \"%s: %d\" % (k, v)\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.tag == \"tag\") and (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "def audit():\n",
    "    #ET.iterparse 不一次全部加载进内存，\n",
    "    for event, elem in ET.iterparse(osm_file):\n",
    "        if is_street_name(elem):\n",
    "            audit_street_type(street_types, elem.attrib['v'])\n",
    "    print_sorted_dict(street_types)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    audit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MongoDB\n",
    "PyMongo translate a python dictinoary into BSON  \n",
    "[PyMongo_doc](https://docs.mongodb.com/getting-started/python/query/)  \n",
    "[PyMongo](http://api.mongodb.com/python/2.8/tutorial.html#bulk-inserts)  \n",
    "[mongodb_manual](https://docs.mongodb.com/manual/reference/command/insert/#dbcmd.insert)  \n",
    "[Query_Project](https://docs.mongodb.com/manual/reference/operator/query/)\n",
    "\n",
    "Methods:get data into MongoDB  \n",
    "1. cleaning data in python, then put into MongoDB  \n",
    "2. Json data,then bulk import MongoDB (using mongoimport)  \n",
    "mongoimport -db dbname -c collectionname --file input-file.json  \n",
    "[mongoimport](https://docs.mongodb.com/manual/reference/program/mongoimport/)  \n",
    "ps:If no hostname and credentials are supplied, mongoimport will try to connect to the default localhost:27017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"插入数据docment  [PyMongo translate a python dictinoary into BSON encoding and send it across to the database]\n",
    "Insert a document into a collection named autos.\n",
    "The operation will create the collection if the collection does not currently exist.\n",
    "\"\"\"\n",
    "from pymongo import MongoClient\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client['example']\n",
    "tesla_s = {...}\n",
    "db.autos.insert(tesla_s) #autos 是一个 collection\n",
    "num_autos = db.autos.find().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"字段筛选\n",
    "\"\"\"\n",
    "\n",
    "# Do not edit code below this line in the online code editor.\n",
    "# Code here is for local use on your own computer.\n",
    "def get_db(db_name):\n",
    "    # For local use\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client[db_name]\n",
    "    return db\n",
    "\n",
    "def find_porsche(db, query):\n",
    "    # For local use\n",
    "    return db.autos.find({\"manufacturer\":\"Porsche\",\"class\":\"mid-size car\"})\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # For local use\n",
    "    db = get_db('examples')\n",
    "    query = porsche_query()\n",
    "    results = find_porsche(db, query)\n",
    "\n",
    "    print \"Printing first 3 results\\n\"\n",
    "    import pprint\n",
    "    for car in results[:3]:\n",
    "        pprint.pprint(car)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#projection 是在query查询结果之后的筛选，query类似where限制条件，projection类似select 的选择字段\n",
    "def find():\n",
    "    query ={\"manufacturer\":\"Porsche\",\"class\":\"mid-size car\"}\n",
    "    projection ={\"_id\":0,\"name\":1} #不返回id，返回name\n",
    "    result = db.autos.find(query,projection)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "operator\n",
    "1. range operator\n",
    "2. if field exists \n",
    "3. regex operator    \n",
    "    Live RegEx tester at [regexpal.com](http://www.regexpal.com/)    \n",
    "    MongoDB [$regex Manual](https://docs.mongodb.com/manual/reference/operator/query/regex/)  \n",
    "    Official Python [Regular Expression HOWTO](https://docs.python.org/2/howto/regex.html)  \n",
    "    [Another good Python Regular Expressions page](https://developers.google.com/edu/python/regular-expressions?csw=1)  \n",
    "4. in / all operator\n",
    "5. query array结构化数据 using scalars标量(like,int,strings)\n",
    "6. Dot notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updates Methods: \n",
    "1. db.cities.save(city)  \n",
    "2. city = db.cities.update({\"name\":\"Munchen\",\"country\":\"Germany\"},{\"$set\":{\"isoCountryCode\":\"DEU\"}}) \n",
    "\n",
    "3. city = db.cities.update({\"name\":\"Munchen\",\"country\":\"Germany\"},{\"$unset\":{\"isoCountryCode\":\"\"}})  \n",
    "4. city = db.cities.update({\"country\":\"Germany\"},{\"$set\":{\"isoCountryCode\":\"DEU\"}},multi = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save\n",
    "city = db.cities.find_one({})\n",
    "city[\"isoCountryCode\"] = \"DEU\"\n",
    "db.cities.save(city)  #可以指定 _id(新增/更新)/系统分配\n",
    "\n",
    "#update  $set\n",
    "city = db.cities.update({\"name\":\"Munchen\",\"country\":\"Germany\"},{\"$set\":{\"isoCountryCode\":\"DEU\"}}) #无isoCountryCode：新增/有：更新\n",
    "\n",
    "#update $unset 有：删除/无：不修改\n",
    "\n",
    "#multi update \n",
    "city = db.cities.update({\"country\":\"Germany\"},{\"$set\":{\"isoCountryCode\":\"DEU\"}},multi = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'db' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-222493aec808>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#DON‘T DO THIS  符合查询条件的docment 全部替换成 {\"isoCountryCode\":\"DEU\"}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"Munchen\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"country\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"Germany\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"isoCountryCode\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"DEU\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'db' is not defined"
     ]
    }
   ],
   "source": [
    "#DON‘T DO THIS  符合查询条件的docment 全部替换成 {\"isoCountryCode\":\"DEU\"}\n",
    "city = db.cities.update({\"name\":\"Munchen\",\"country\":\"Germany\"},{\"isoCountryCode\":\"DEU\"}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db.cities.remove() \n",
    "db.cities.remove({\"name\":\"Chicago\"}) #删除满足条件的documents\n",
    "db.cities.drop() #删除整个collection和相关的元数据，比如索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### aggregation pipeline ：operate one by one\n",
    "[mongodb_aggregation_operator](https://docs.mongodb.com/manual/reference/operator/aggregation-pipeline/)  \n",
    "\n",
    "operation|fuction\n",
    "--|--|\n",
    "\\$group| divide\n",
    "\\$sort  |sort\n",
    "\\$project  |shaping: pass specified fields(existing or newly computed) to the next stage.  \n",
    "\\$match |filtering\n",
    "\\$skip  |skip top\n",
    "\\$limit  |get top\n",
    "\\$unwind 可以将一个document按array中不同的value分解成多个documents  \n",
    "\n",
    "[group operators](https://docs.mongodb.com/manual/reference/operator/aggregation/group/)  \n",
    "\n",
    "\\$sum  \n",
    "\\$first  \n",
    "\\$last  \n",
    "\\$max  \n",
    "\\$min  \n",
    "\\$avg  \n",
    "\\$push  \n",
    "\\$addToSet  \n",
    "\n",
    "Conditional Aggregation Operators\n",
    "ifNull:null、missing，返回替代值；否则，返回原value\n",
    "\n",
    "\n",
    "     {\n",
    "         \"$project\": {\n",
    "            \"item\": 1,\n",
    "            \"description\": { \"$ifNull\": [ \"$description\", \"Unspecified\" ] }\n",
    "         }\n",
    "      }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#$group;$month等 $表示调用函数\n",
    "# _id;totalPrice;averageQuantity;count等 输出的field\n",
    "#_id 分组的依据，不可省略，可为NULL或固值（在对整体求平均时）\n",
    "#$group  不带order功能\n",
    "db.sales.aggregate(\n",
    "   [\n",
    "      {\n",
    "        \"$group\" : {\n",
    "           \"_id\" : { \"month\": { \"$month\": \"$date\" }, \"day\": { \"$dayOfMonth\": \"$date\" }, \"year\": { \"$year\": \"$date\" } },\n",
    "           \"totalPrice\": { \"$sum\": { \"$multiply\": [ \"$price\", \"$quantity\" ] } },\n",
    "           \"averageQuantity\": { \"$avg\": \"$quantity\" },\n",
    "           \"count\": { \"$sum\": 1 }\n",
    "        }\n",
    "      }\n",
    "      {\n",
    "        \"$sort\" :{ \"averageQuantity\": -1}\n",
    "        }\n",
    "   ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# $ 用于1.表示调用函数 2.取该filed name的value，而非字符串\n",
    "# \"screen_name\":\"$user.screen_name\" 第一个screen_name 表示结果的field name（不存在，则新建）\n",
    "\n",
    "pipeline = [\n",
    "                {\"$match\":{\"user.time_zone\":\"Brasilia\",\n",
    "                           \"user.statuses_count\":{\"$gte\":100},\n",
    "                            \"user.followers_count\":{\"$gt\":0}}},\n",
    "                {\"$project\":{\"screen_name\":\"$user.screen_name\",\n",
    "                             \"tweets\":\"$user.statuses_count\",\n",
    "                             \"followers\":\"$user.followers_count\"}},\n",
    "                {\"$sort\":{\"followers\":-1}},\n",
    "                {\"$limit\":1}\n",
    "                ]\n",
    "#match  multi conditions\n",
    "{\"$match\":{\"country\":\"India\",\"lon\":{\"$gte\":75,\"$let\":80}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    pipeline = [{\"$match\":{\"country\":\"India\"}},\n",
    "                {\"$unwind\":\"$isPartOf\"},\n",
    "                {\"$group\":{\"_id\":\"$isPartOf\",\"count\":{\"$sum\":1}}},\n",
    "                {\"$sort\":{\"count\":-1}},\n",
    "                {\"$limit\":1} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# $addToSet\n",
    "{\n",
    "       $group:\n",
    "         {\n",
    "           _id: { day: { $dayOfYear: \"$date\"}, year: { $year: \"$date\" } },\n",
    "           itemsSold: { $addToSet: \"$item\" }\n",
    "         }\n",
    "     }\n",
    "\n",
    "#output\n",
    "{ \"_id\" : { \"day\" : 46, \"year\" : 2014 }, \"itemsSold\" : [ \"xyz\", \"abc\" ] }\n",
    "{ \"_id\" : { \"day\" : 34, \"year\" : 2014 }, \"itemsSold\" : [ \"xyz\", \"jkl\" ] }\n",
    "{ \"_id\" : { \"day\" : 1, \"year\" : 2014 }, \"itemsSold\" : [ \"abc\" ] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# $push\n",
    "{\n",
    "       $group:\n",
    "         {\n",
    "           _id: { day: { $dayOfYear: \"$date\"}, year: { $year: \"$date\" } },\n",
    "           itemsSold: { $push:  { item: \"$item\", quantity: \"$quantity\" } }\n",
    "         }\n",
    "     }\n",
    "\n",
    "#output\n",
    "{\n",
    "   \"_id\" : { \"day\" : 46, \"year\" : 2014 },\n",
    "   \"itemsSold\" : [\n",
    "      { \"item\" : \"abc\", \"quantity\" : 10 },\n",
    "      { \"item\" : \"xyz\", \"quantity\" : 10 },\n",
    "      { \"item\" : \"xyz\", \"quantity\" : 5 },\n",
    "      { \"item\" : \"xyz\", \"quantity\" : 10 }\n",
    "   ]\n",
    "}\n",
    "{\n",
    "   \"_id\" : { \"day\" : 34, \"year\" : 2014 },\n",
    "   \"itemsSold\" : [\n",
    "      { \"item\" : \"jkl\", \"quantity\" : 1 },\n",
    "      { \"item\" : \"xyz\", \"quantity\" : 5 }\n",
    "   ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多个运算阶段"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 附录\n",
    "### josn data <--> python data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"json 模块提供了一种很简单的方式来编码和解码JSON数据。 \n",
    "其中两个主要的函数是 json.dumps() 和 json.loads(),要比其他序列化函数库如pickle的接口少得多。 \"\"\"\n",
    "#下面演示如何将一个Python数据结构转换为JSON：\n",
    "import json\n",
    "\n",
    "data = {\n",
    "    'name' : 'ACME',\n",
    "    'shares' : 100,\n",
    "    'price' : 542.23\n",
    "}\n",
    "\n",
    "json_str = json.dumps(data)\n",
    "#下面演示如何将一个JSON编码的字符串转换回一个Python数据结构：\n",
    "data = json.loads(json_str)\n",
    "\n",
    "\n",
    "#如果你要处理的是文件而不是字符串，你可以使用 json.dump() 和 json.load() 来编码和解码JSON数据。例如：\n",
    "# Writing JSON data\n",
    "with open('data.json', 'w') as f:\n",
    "    json.dump(data, f)\n",
    "\n",
    "# Reading data back\n",
    "with open('data.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re 正则表达式\n",
    "\n",
    "[Python正则表达式指南](http://www.cnblogs.com/huxi/archive/2010/07/04/1771073.html)  \n",
    "[官方re](https://docs.python.org/2/library/re.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#设置一种匹配模式，再去匹配\n",
    "The sequence\n",
    "prog = re.compile(pattern)\n",
    "result = prog.match(string)\n",
    "\n",
    "#直接用一种模式匹配，一次性\n",
    "is equivalent to\n",
    "result = re.match(pattern, string)\n",
    "\n",
    "#不含字母数字\n",
    "if (re.match('[^a-z0-9]', row[k], re.IGNORECASE)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 4), ('p', 2), ('s', 4), ('m', 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "s = 'mississippi'\n",
    "d = defaultdict(int)\n",
    "for k in s:\n",
    "     d[k] += 1\n",
    "d.items()\n",
    "#[('i', 4), ('p', 2), ('s', 4), ('m', 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isaac Newton\n",
      "Isaac\n",
      "Newton\n",
      "('Isaac', 'Newton')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "m = re.match(r\"(\\w+) (\\w+)\", \"Isaac Newton, physicist\")\n",
    "print m.group(0)       # The entire match\n",
    "#'Isaac Newton'\n",
    "print m.group(1)       # The first parenthesized subgroup.\n",
    "#'Isaac'\n",
    "print m.group(2)       # The second parenthesized subgroup.\n",
    "#'Newton'\n",
    "print m.group(1, 2)    # Multiple arguments give us a tuple.\n",
    "#('Isaac', 'Newton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Isaac Newton'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get sample_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import xml.etree.ElementTree as ET  # Use cElementTree or lxml if too slow\n",
    "\n",
    "OSM_FILE = \"some_osm.osm\"  # Replace this with your osm file\n",
    "SAMPLE_FILE = \"sample.osm\"\n",
    "\n",
    "k = 10 # Parameter: take every k-th top level element\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\n",
    "\n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",
    "    \"\"\"\n",
    "    context = iter(ET.iterparse(osm_file, events=('start', 'end')))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "with open(SAMPLE_FILE, 'wb') as output:\n",
    "    output.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    output.write('<osm>\\n  ')\n",
    "\n",
    "    # Write every kth top level element\n",
    "    for i, element in enumerate(get_element(OSM_FILE)):\n",
    "        if i % k == 0:\n",
    "            output.write(ET.tostring(element, encoding='utf-8'))\n",
    "\n",
    "    output.write('</osm>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boy', 'gril', 'w'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = set(['boy'])\n",
    "a.add('gril')\n",
    "a.add('w')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b', 'gril', 'o', 'w', 'y'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = set('boy')\n",
    "a.add('gril')\n",
    "a.add('w')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boy', 'girl'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = set()\n",
    "a.add(\"girl\")\n",
    "a.add('boy')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "area = '{3.43173e+07|3.432e+07}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34320000.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([float(a) for a in (area[1:-1].split(\"|\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'a'.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ave.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "s = ' acewuo ave.'\n",
    "street_type = re.compile(r'\\b\\S+\\.?$',re.IGNORECASE) \n",
    "m = street_type.search(s)\n",
    "m.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
