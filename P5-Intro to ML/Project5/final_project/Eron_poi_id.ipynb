{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "In 2000, Enron was one of the largest companies in the United States. By 2002, it had collapsed into bankruptcy due to widespread corporate fraud.  \n",
    "In the resulting Federal investigation, a significant amount of typically confidential information entered into the public record, including tens of thousands of emails and detailed financial data for top executives.   \n",
    "   \n",
    "In this project,I'm going to build a person of interest identifier based on financial and email data made public as a result of the Enron scandal. \n",
    "\n",
    "Model performance：Accuracy: 0.85173; Precision: 0.43736; Recall: 0.39100;F1: 0.41288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/weidian1/Documents/GitHub/Udacity-DAND/P5-Intro to ML/Project5/final_project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/weidian1/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data points:  143\n",
      "Number of POI: 18, no. of non-POI: 125\n",
      "Number of features used:  21\n"
     ]
    }
   ],
   "source": [
    "# 异常值， 去掉total\n",
    "def remove_outerliers(data_dict,names):\n",
    "    for name in names:\n",
    "        data_dict.pop(name, 0)\n",
    "    return data_dict\n",
    "\n",
    "def dateset_summary(data_dic):\n",
    "    n_poi,n_non_poi = 0,0\n",
    "    for key in data_dict.keys():\n",
    "        if data_dict[key]['poi']==1:\n",
    "            n_poi+=1\n",
    "        else:\n",
    "            n_non_poi+=1        \n",
    "    print \"Total number of data points: \", len(data_dict)\n",
    "    print \"Number of POI: %d, no. of non-POI: %d\"% (n_poi, n_non_poi)\n",
    "    print \"Number of features used: \",len(data_dict['METTS MARK']) # randomly pick one name, get the number of features.\n",
    "\n",
    "outliers_names =  ['TOTAL','THE TRAVEL AGENCY IN THE PARK', 'LOCKHART EUGENE E']\n",
    "\n",
    "data_dict = remove_outerliers(data_dict,outliers_names)\n",
    "dateset_summary(dateset_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salary                        0\n",
      "to_messages                  57\n",
      "deferral_payments             0\n",
      "total_payments                0\n",
      "exercised_stock_options       0\n",
      "bonus                         0\n",
      "restricted_stock              0\n",
      "shared_receipt_with_poi      57\n",
      "restricted_stock_deferred     0\n",
      "total_stock_value             0\n",
      "expenses                      0\n",
      "loan_advances                 0\n",
      "from_messages                57\n",
      "other                         0\n",
      "from_this_person_to_poi      57\n",
      "poi                           0\n",
      "director_fees                 0\n",
      "deferred_income               0\n",
      "long_term_incentive           0\n",
      "email_address                32\n",
      "from_poi_to_this_person      57\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 查看缺失情况；poi转为int型\n",
    "data_df = pd.DataFrame.from_dict(data_dict,orient='index')\n",
    "financial_features = ['salary', 'deferral_payments', 'total_payments', 'loan_advances', 'bonus'\n",
    "                 , 'restricted_stock_deferred', 'deferred_income', 'total_stock_value', 'expenses'\n",
    "                 , 'exercised_stock_options', 'other', 'long_term_incentive', 'restricted_stock'\n",
    "                 , 'director_fees']\n",
    "email_fatures = ['to_messages','from_poi_to_this_person', 'from_messages', 'from_this_person_to_poi'\n",
    "                 , 'shared_receipt_with_poi']  # remove 'email_address'\n",
    "def NaNs_to_0s(col):\n",
    "    return [0 if ele=='NaN' else ele for ele in col]\n",
    "\n",
    "def NaNs_to_None(col):\n",
    "    return [None if ele=='NaN' else ele for ele in col]\n",
    "\n",
    "def count_NAN(col):\n",
    "    return sum(1 for ele in col if ele == 'NaN')\n",
    "\n",
    "data_df[financial_features] = data_df[financial_features].apply(NaNs_to_0s, axis=0)\n",
    "data_df['poi'] = data_df['poi'].astype(int)\n",
    "print data_df.apply(count_NAN, axis=0)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "make new feature \n",
    "1. 'email_features_miss': if email_features is missing. next step, missing email_features will be\n",
    "transformed to 0, which should by different from missing financial features.   \n",
    "2. poi_rate_to_messages = from_this_person_to_poi/to_messages  \n",
    "3. poi_rate_from_messages = from_poi_to_this_person/from_messages\n",
    "\"\"\"\n",
    "\n",
    "data_df['email_features_miss'] = data_df.apply(lambda x: 1 if x[\"to_messages\"]=='NaN' else 0, axis = 1)\n",
    "\n",
    "data_df['poi_rate_to_messages'] = data_df.apply(lambda x: 0 if x['from_this_person_to_poi']=='NaN' \\\n",
    "                                                else x['from_this_person_to_poi']*1.0/x['to_messages'], axis=1)\n",
    "\n",
    "data_df['poi_rate_from_messages'] = data_df.apply(lambda x: 0 if x['from_poi_to_this_person']=='NaN'\\\n",
    "                                                  else x['from_poi_to_this_person']*1.0/x['from_messages'], axis =1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143, 24)\n",
      "['salary' 'to_messages' 'deferral_payments' 'total_payments'\n",
      " 'exercised_stock_options' 'bonus' 'restricted_stock'\n",
      " 'shared_receipt_with_poi' 'restricted_stock_deferred' 'total_stock_value'\n",
      " 'expenses' 'loan_advances' 'from_messages' 'other'\n",
      " 'from_this_person_to_poi' 'poi' 'director_fees' 'deferred_income'\n",
      " 'long_term_incentive' 'email_address' 'from_poi_to_this_person'\n",
      " 'email_features_miss' 'poi_rate_to_messages' 'poi_rate_from_messages']\n"
     ]
    }
   ],
   "source": [
    "print data_df.shape#columns.values; 24个变量\n",
    "print data_df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store to my_dataset, my_feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 full_columns_name: ['poi', 'salary', 'to_messages', 'deferral_payments', 'total_payments', 'exercised_stock_options', 'bonus', 'restricted_stock', 'shared_receipt_with_poi', 'restricted_stock_deferred', 'total_stock_value', 'expenses', 'loan_advances', 'from_messages', 'other', 'from_this_person_to_poi', 'director_fees', 'deferred_income', 'long_term_incentive', 'from_poi_to_this_person', 'email_features_miss', 'poi_rate_to_messages', 'poi_rate_from_messages']\n"
     ]
    }
   ],
   "source": [
    "features_full_list = list(data_df.columns.values)\n",
    "features_full_list.remove('email_address')\n",
    "features_full_list.remove('poi')\n",
    "features_full_list = ['poi'] +features_full_list\n",
    "data_full_dic = data_df.to_dict(orient='index')\n",
    "print (\"%d full_columns_name: %s\" % (len(features_full_list), features_full_list))\n",
    "    \n",
    "def get_features_labels(my_dataset,features_list):\n",
    "    #Extract features and labels from dataset for local testing\n",
    "    data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "    # TODO: Shuffle the data\n",
    "    from sklearn.utils import shuffle\n",
    "    features, labels = shuffle(features, labels, random_state=1)\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "    return features, labels\n",
    "\n",
    "features, labels = get_features_labels(data_full_dic,features_full_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_full_dic\n",
    "my_feature_list = features_full_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Several Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.79713\tPrecision: 0.23781\tRecall: 0.23650\tF1: 0.23715\tF2: 0.23676\n",
      "\tTotal predictions: 15000\tTrue positives:  473\tFalse positives: 1516\tFalse negatives: 1527\tTrue negatives: 11484\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tester\n",
    "from tester import test_classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tester.test_classifier(DecisionTreeClassifier(), my_dataset, my_feature_list,folds = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.73900\tPrecision: 0.22604\tRecall: 0.39500\tF1: 0.28753\tF2: 0.34363\n",
      "\tTotal predictions: 15000\tTrue positives:  790\tFalse positives: 2705\tFalse negatives: 1210\tTrue negatives: 10295\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "tester.test_classifier(GaussianNB(), my_dataset, my_feature_list,folds = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.83400\tPrecision: 0.34572\tRecall: 0.27450\tF1: 0.30602\tF2: 0.28630\n",
      "\tTotal predictions: 15000\tTrue positives:  549\tFalse positives: 1039\tFalse negatives: 1451\tTrue negatives: 11961\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "tester.test_classifier(AdaBoostClassifier(), my_dataset, my_feature_list,folds = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifier model and tune parameters\n",
    "Using AdaBoost, GridSearchCv, Pipeline, StratifiedShuffleSplit.    \n",
    "define a new metric function 'scorer_r_p' ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/weidian1/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/weidian1/anaconda/lib/python2.7/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [11] are constant.\n",
      "  UserWarning)\n",
      "/Users/weidian1/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/weidian1/anaconda/lib/python2.7/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [11] are constant.\n",
      "  UserWarning)\n",
      "/Users/weidian1/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/weidian1/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/weidian1/anaconda/lib/python2.7/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [11] are constant.\n",
      "  UserWarning)\n",
      "/Users/weidian1/anaconda/lib/python2.7/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [11] are constant.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedShuffleSplit(labels=[ 0.  0. ...,  0.  0.], n_iter=50, test_size=0.2, random_state=42),\n",
       "       error_score='raise',\n",
       "       estimator=Pipeline(steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selector', SelectKBest(k=10, score_func=<function f_classif at 0x113c272a8>)), ('classifier', AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=32))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'classifier__learning_rate': [1, 1.5, 2], 'classifier__base_estimator': [DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split...100], 'selector__score_func': [<function chi2 at 0x113c27398>, <function f_classif at 0x113c272a8>]},\n",
       "       pre_dispatch='2*n_jobs', refit=True,\n",
       "       scoring=<function scorer_r_p at 0x11435ec80>, verbose=0)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest,chi2,f_classif\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, fbeta_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def scorer_r_p(estimator, features_test, labels_test):\n",
    "    labels_pred = estimator.predict(features_test)\n",
    "#     pre= precision_score(labels_test, labels_pred)\n",
    "#     rec = recall_score(labels_test, labels_pred)\n",
    "#     if pre>0.3 and rec>0.3:\n",
    "#        return fbeta_score(labels_test, labels_pred, beta=1.1)\n",
    "#     elif  pre>0.3 and rec<0.3:\n",
    "#        return 0.3\n",
    "#     elif rec >0.3 and pre<0.3:\n",
    "#        return 0.3\n",
    "    return f1_score(labels_test, labels_pred)\n",
    "\n",
    "sss = StratifiedShuffleSplit(labels, n_iter=50, test_size = 0.2, random_state=42)\n",
    "pipe = Pipeline([('scaler',MinMaxScaler(feature_range=(0,1))), \n",
    "                ('selector', SelectKBest()),\n",
    "                  ('classifier', AdaBoostClassifier(random_state=32))])\n",
    "param_grid = dict( selector__score_func=[chi2,f_classif],\n",
    "                  selector__k=range(4,6),\n",
    "                 classifier__n_estimators=[50,80,100]\n",
    "                 ,classifier__learning_rate=[1,1.5,2]\n",
    "                ,classifier__base_estimator=[DecisionTreeClassifier(min_samples_split=2)\\\n",
    "                                                , DecisionTreeClassifier(min_samples_split=3)\\\n",
    "                                                , DecisionTreeClassifier(min_samples_split=4)]\\\n",
    "                 )\n",
    "clf_grid = GridSearchCV(pipe, param_grid, scoring=scorer_r_p, cv=sss, n_jobs=-1)\n",
    "clf_grid.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))),\n",
       " ('selector', SelectKBest(k=4, score_func=<function chi2 at 0x113c27398>)),\n",
       " ('classifier', AdaBoostClassifier(algorithm='SAMME.R',\n",
       "            base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "              min_samples_split=4, min_weight_fraction_leaf=0.0,\n",
       "              presort=False, random_state=None, splitter='best'),\n",
       "            learning_rate=1.5, n_estimators=50, random_state=32))]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_grid.best_estimator_.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40955067155067165"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_grid.best_score_ #f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))),\n",
       " ('selector', SelectKBest(k=4, score_func=<function chi2 at 0x113c27398>)),\n",
       " ('classifier', AdaBoostClassifier(algorithm='SAMME.R',\n",
       "            base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "              min_samples_split=4, min_weight_fraction_leaf=0.0,\n",
       "              presort=False, random_state=None, splitter='best'),\n",
       "            learning_rate=1.5, n_estimators=50, random_state=32))]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_grid.best_estimator_.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39107475361810556"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Given TestSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selector', SelectKBest(k=5, score_func=<function chi2 at 0x113c27398>)), ('classifier', AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "   ...ndom_state=None, splitter='best'),\n",
      "          learning_rate=1.5, n_estimators=100, random_state=32))])\n",
      "\tAccuracy: 0.85800\tPrecision: 0.45638\tRecall: 0.34000\tF1: 0.38968\tF2: 0.35827\n",
      "\tTotal predictions: 15000\tTrue positives:  680\tFalse positives:  810\tFalse negatives: 1320\tTrue negatives: 12190\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tester\n",
    "from tester import test_classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "tester.test_classifier(clf_grid.best_estimator_, my_dataset, my_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selector', SelectKBest(k=4, score_func=<function chi2 at 0x113c27398>)), ('classifier', AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "   ...andom_state=None, splitter='best'),\n",
      "          learning_rate=1.5, n_estimators=50, random_state=32))])\n",
      "\tAccuracy: 0.85187\tPrecision: 0.43771\tRecall: 0.39000\tF1: 0.41248\tF2: 0.39869\n",
      "\tTotal predictions: 15000\tTrue positives:  780\tFalse positives: 1002\tFalse negatives: 1220\tTrue negatives: 11998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tester.test_classifier(clf_grid.best_estimator_, my_dataset, my_feature_list,folds=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('classifier', AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_s...andom_state=None, splitter='best'),\n",
      "          learning_rate=1.5, n_estimators=50, random_state=32))])\n",
      "\tAccuracy: 0.87327\tPrecision: 0.53263\tRecall: 0.40400\tF1: 0.45948\tF2: 0.42450\n",
      "\tTotal predictions: 15000\tTrue positives:  808\tFalse positives:  709\tFalse negatives: 1192\tTrue negatives: 12291\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#evaluate performance of model added three new feature\n",
    "clf_addNew = Pipeline(steps=[clf_grid.best_estimator_.steps[i] for i in [0,2]])      \n",
    "my_feature_list_addNew = ['poi','exercised_stock_options','loan_advances','total_stock_value'\\\n",
    "                          ,'bonus','email_features_miss','poi_rate_to_messages','poi_rate_from_messages']\n",
    "tester.test_classifier(clf_addNew, my_dataset,my_feature_list_addNew )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('classifier', AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_s...andom_state=None, splitter='best'),\n",
      "          learning_rate=1.5, n_estimators=50, random_state=32))])\n",
      "\tAccuracy: 0.83367\tPrecision: 0.30435\tRecall: 0.19250\tF1: 0.23583\tF2: 0.20777\n",
      "\tTotal predictions: 15000\tTrue positives:  385\tFalse positives:  880\tFalse negatives: 1615\tTrue negatives: 12120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with all features\n",
    "clf_addNew = Pipeline(steps=[clf_grid.best_estimator_.steps[i] for i in [0,2]])      \n",
    "tester.test_classifier(clf_addNew, my_dataset,my_feature_list )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scores\n",
    "Final classifier uses top 4 features, print out the feature scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Scale\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "features_scaled = scaler.fit_transform(X=features, y=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.05278674e+00,   4.36397769e-01,   6.06966069e-02,\n",
       "         2.78477884e+00,   6.84550934e+00,   5.12075414e+00,\n",
       "         5.89535349e-01,   2.43221987e+00,   3.50676503e-03,\n",
       "         5.47661010e+00,   1.48610337e+00,   6.68878174e+00,\n",
       "         6.87385422e-02,   1.71595053e+00,   1.00080764e+00,\n",
       "         1.50113085e+00,   3.40099218e-01,   2.53848503e+00,\n",
       "         1.37005929e+00,   1.60714230e+00,   1.26978364e+00,\n",
       "         1.78530514e+00])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#KBest\n",
    "from sklearn.feature_selection import SelectKBest,chi2\n",
    "kb = SelectKBest(score_func=chi2,k=4)\n",
    "kb.fit_transform(features_scaled,labels)\n",
    "kb.scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature 4 exercised_stock_options (6.845509)\n",
      "2. feature 11 loan_advances (6.688782)\n",
      "3. feature 9 total_stock_value (5.476610)\n",
      "4. feature 5 bonus (5.120754)\n",
      "5. feature 0 salary (3.052787)\n",
      "6. feature 3 total_payments (2.784779)\n",
      "7. feature 17 long_term_incentive (2.538485)\n",
      "8. feature 7 shared_receipt_with_poi (2.432220)\n",
      "9. feature 21 poi_rate_from_messages (1.785305)\n",
      "10. feature 13 other (1.715951)\n",
      "11. feature 19 email_features_miss (1.607142)\n",
      "12. feature 15 director_fees (1.501131)\n",
      "13. feature 10 expenses (1.486103)\n",
      "14. feature 18 from_poi_to_this_person (1.370059)\n",
      "15. feature 20 poi_rate_to_messages (1.269784)\n",
      "16. feature 14 from_this_person_to_poi (1.000808)\n",
      "17. feature 6 restricted_stock (0.589535)\n",
      "18. feature 1 to_messages (0.436398)\n",
      "19. feature 16 deferred_income (0.340099)\n",
      "20. feature 12 from_messages (0.068739)\n",
      "21. feature 2 deferral_payments (0.060697)\n",
      "22. feature 8 restricted_stock_deferred (0.003507)\n"
     ]
    }
   ],
   "source": [
    "importances = kb.scores_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(features_scaled .shape[1]):\n",
    "    print(\"%d. feature %d %s (%f)\" % (f + 1, indices[f],features_full_list[1:][indices[f]],importances[indices[f]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest,chi2\n",
    "kb = SelectKBest(score_func=chi2,k=4)\n",
    "kb.fit_transform(features_scaled,labels)\n",
    "kb.scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dump \n",
    "classifier, dataset, and features_list. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tester import dump_classifier_and_data\n",
    "dump_classifier_and_data(clf_grid.best_estimator_, my_dataset, my_feature_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
