{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### Load the dictionary containing the dataset\n",
    "enron_dict = pickle.load(open(\"final_project_dataset.pkl\", \"r\") )\n",
    "\n",
    "### Explore the data\n",
    "## what are the features?\n",
    "features_explore = enron_dict[enron_dict.keys()[0]].keys()\n",
    "#print features_explore \n",
    "\n",
    "## How many features?\n",
    "print len(features_explore) # 21 features\n",
    "\n",
    "## How many data points, NaNs, POIs etc?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create eda data frame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "### The first feature must be \"poi\".\n",
    "features_list = ['poi', 'salary', 'to_messages', 'deferral_payments', 'total_payments', \\\n",
    "'exercised_stock_options', 'bonus', 'restricted_stock', 'shared_receipt_with_poi', \\\n",
    "'restricted_stock_deferred', 'total_stock_value', 'expenses', 'loan_advances', 'from_messages', \\\n",
    "'other', 'from_this_person_to_poi', 'director_fees', 'deferred_income', 'long_term_incentive', \\\n",
    "'from_poi_to_this_person'] \n",
    "#, 'email_address' removed\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "### Outliers\n",
    "\n",
    "## Remove Outliers entry from dictionary\n",
    "\n",
    "outlier_keys = ['TOTAL', 'THE TRAVEL AGENCY IN THE PARK', 'LOCKHART EUGENE E']\n",
    "for key in outlier_keys:\n",
    "        enron_dict.pop(key, 0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['poi', 'salary', 'to_messages', 'deferral_payments', 'total_payments', 'exercised_stock_options', 'bonus', 'restricted_stock', 'shared_receipt_with_poi', 'restricted_stock_deferred', 'total_stock_value', 'expenses', 'loan_advances', 'from_messages', 'other', 'from_this_person_to_poi', 'director_fees', 'deferred_income', 'long_term_incentive', 'from_poi_to_this_person', 'poi_interaction', 'remuneration']\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "### Task 3: Create new feature(s)\n",
    "\n",
    "\n",
    "# add two new features\n",
    "\n",
    "# poi_interaction == total number of emails to and from a POI / \n",
    "#the total number of emails sent or received. \n",
    "#def add_financial_aggregate(my_dataset, my_feature_list)\n",
    "\n",
    "# define email_fields\n",
    "email_fields = ['to_messages', 'from_messages',\n",
    "          'from_poi_to_this_person', 'from_this_person_to_poi']\n",
    "# loop through data dict\n",
    "for record in enron_dict:\n",
    "    # set person as a value of a record \n",
    "    person = enron_dict[record]\n",
    "    # new variable\n",
    "    is_valid = True\n",
    "    # loop through email fields\n",
    "    for field in email_fields:\n",
    "        # check if field value in enron_dict is NaN\n",
    "        if person[field] == 'NaN':\n",
    "            # set is_valid as false\n",
    "            is_valid = False\n",
    "    # check for when is_valid is true        \n",
    "    if is_valid:\n",
    "        # calculate field value\n",
    "        total_messages = person['to_messages'] +\\\n",
    "                         person['from_messages']\n",
    "        poi_messages = person['from_poi_to_this_person'] +\\\n",
    "                       person['from_this_person_to_poi']\n",
    "        person['poi_interaction'] = float(poi_messages) / total_messages\n",
    "    else:\n",
    "        # set value as NaN\n",
    "        person['poi_interaction'] = 'NaN'\n",
    "# add to dictionary        \n",
    "features_list += ['poi_interaction']\n",
    "\n",
    "# remuneration == total payments + total stock value \n",
    "# to capture total wealth\n",
    "#def add_financial_aggregate(data_dict, features_list):\n",
    "    \n",
    "financial_fields = ['total_payments', 'total_stock_value']\n",
    "for record in enron_dict:\n",
    "    person = enron_dict[record]\n",
    "    is_valid = True\n",
    "    for field in financial_fields:\n",
    "        if person[field] == 'NaN':\n",
    "            is_valid = False\n",
    "    if is_valid:\n",
    "        # sum every field defined in financial_fields\n",
    "        person['remuneration'] = sum([person[field] for field in financial_fields])\n",
    "    else:\n",
    "        person['remuneration'] = 'NaN'\n",
    "features_list += ['remuneration']\n",
    "\n",
    "print features_list\n",
    "\n",
    "print len(features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Prior to training the machine learning algorithm classifiers, \n",
    "#I scaled all features using a min-max scaler. \n",
    "#This was vitally important, as the features had different units \n",
    "#(e.g. # of email messages and USD) and varied significantly by several orders of magnitude. \n",
    "#Feature-scaling ensured that for the applicable classifiers, the features would be weighted evenly.\n",
    "\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = enron_dict\n",
    "my_feature_list = features_list\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, my_feature_list)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "#print labels, features\n",
    "\n",
    "# Apply min-max feature scaling\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "\n",
    "## Apply PCA to pre-process features to be simplified\n",
    "#from sklearn import decomposition\n",
    "#\n",
    "##n_components = 21\n",
    "#perc_var = .95\n",
    "#pca = decomposition.PCA(n_components = perc_var, whiten = True).fit(features)\n",
    "#features = pca.transform(features)\n",
    "##print {\"Explained Variance Ratio\": pca.explained_variance_ratio_}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to_messages 0.0\n",
      "deferral_payments 0.0\n",
      "restricted_stock_deferred 0.0\n",
      "total_stock_value 0.0\n",
      "loan_advances 0.0\n",
      "director_fees 0.0\n",
      "deferred_income 0.0\n",
      "from_poi_to_this_person 0.0\n",
      "poi_interaction 0.0\n",
      "remuneration 0.0\n",
      "{'salary': 0.052962962962962962, 'total_payments': 0.14476543209876541, 'bonus': 0.1091318137409264, 'shared_receipt_with_poi': 0.057777777777777775, 'exercised_stock_options': 0.19984012789768188, 'from_messages': 0.10375925925925925, 'other': 0.030023449025545469, 'from_this_person_to_poi': 0.0073448637316560501, 'long_term_incentive': 0.10895238095238094, 'expenses': 0.064896989949621606, 'restricted_stock': 0.12054494260342233}\n",
      "['poi', 'salary', 'total_payments', 'bonus', 'shared_receipt_with_poi', 'exercised_stock_options', 'from_messages', 'other', 'from_this_person_to_poi', 'long_term_incentive', 'expenses', 'restricted_stock']\n"
     ]
    }
   ],
   "source": [
    "# Use Decision Tree for feature selection and update features\n",
    "from sklearn import tree\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(features, labels)\n",
    "tree_scores = zip(features_list[1:], clf.feature_importances_)\n",
    "tree_dict = {}\n",
    "i = 0\n",
    "for line in tree_scores:\n",
    "    feat_, score_ = tree_scores[i]\n",
    "    tree_dict[feat_] = score_\n",
    "    i += 1\n",
    "    if score_ == 0.0:\n",
    "        print feat_, score_\n",
    "        del tree_dict[feat_]\n",
    "print tree_dict\n",
    "\n",
    "# Update feature list\n",
    "target_label = 'poi'\n",
    "features_list = [target_label] + tree_dict.keys()\n",
    "print features_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = enron_dict\n",
    "my_feature_list = features_list\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, my_feature_list)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "#print labels, features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['poi',\n",
       " 'salary',\n",
       " 'total_payments',\n",
       " 'bonus',\n",
       " 'shared_receipt_with_poi',\n",
       " 'exercised_stock_options',\n",
       " 'from_messages',\n",
       " 'other',\n",
       " 'from_this_person_to_poi',\n",
       " 'long_term_incentive',\n",
       " 'expenses',\n",
       " 'restricted_stock']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "# get K-best features\n",
    "from sklearn import feature_selection\n",
    "\n",
    "def get_k_best(data_dict, features_list, k):\n",
    "    # runs scikit-learn's SelectKBest feature selection\n",
    "    # returns dict where keys=features, values=scores\n",
    "    data = featureFormat(enron_dict, features_list)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "\n",
    "    k_best = feature_selection.SelectKBest(k=k)\n",
    "    k_best.fit(features, labels)\n",
    "    scores = k_best.scores_\n",
    "    unsorted_pairs = zip(features_list[1:], scores)\n",
    "    sorted_pairs = list(reversed(sorted(unsorted_pairs, key=lambda x: x[1])))\n",
    "    k_best_features = dict(sorted_pairs[:k])\n",
    "\n",
    "    return k_best_features\n",
    "\n",
    "num_features = len(tree_dict)\n",
    "best_features = get_k_best(my_dataset, my_feature_list, num_features)\n",
    "print best_features\n",
    "my_feature_list = [target_label] + best_features.keys()\n",
    "\n",
    "#print my_feature_list\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=5,\n",
      "          n_estimators=50, random_state=None)\n",
      "\n",
      "Processing..................................................complete.\n",
      "\n",
      "accuracy: 0.120689655172\n",
      "precision: 1.0\n",
      "recall:    0.120689655172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.12068965517241378, 1.0, 0.12068965517241378)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "\n",
    "### Logistic Regression Classifier\n",
    "from sklearn import linear_model\n",
    "log_clf = linear_model.LogisticRegression()\n",
    "\n",
    "### Stochastic Gradient Descent - Logistic Regression\n",
    "sgd_clf = linear_model.SGDClassifier(loss='log')\n",
    "\n",
    "### Random Forest\n",
    "from sklearn import ensemble\n",
    "rf_clf = ensemble.RandomForestClassifier()\n",
    "\n",
    "### Decision Tree\n",
    "dt_clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "### Adaboost\n",
    "#ada_clf = ensemble.AdaBoostClassifier(learning_rate=5)\n",
    "\n",
    "\n",
    "from numpy import mean\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "def evaluate_clf(clf, features, labels, trials = 500, test_size = 0.4, random_state = 42):\n",
    "    print clf\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    first = True\n",
    "    for trial in range(trials):\n",
    "        features_train, features_test, labels_train, labels_test = \\\n",
    "            model_selection.train_test_split(features, labels, test_size = test_size, random_state = random_state)\n",
    "        # fit the data\n",
    "        clf.fit(features_train, labels_train)\n",
    "\n",
    "        predictions = clf.predict(features_test)\n",
    "        accuracy.append(metrics.accuracy_score(predictions,labels_test))\n",
    "        precision.append(metrics.precision_score(predictions, labels_test))\n",
    "        recall.append(metrics.recall_score(predictions, labels_test))\n",
    "        if (trial % 10 == 0):\n",
    "            if first:\n",
    "                sys.stdout.write('\\nProcessing')\n",
    "            sys.stdout.write('.')\n",
    "            sys.stdout.flush()\n",
    "            first = False    \n",
    "    print \"complete.\\n\"\n",
    "    print \"accuracy: {}\".format(np.mean(accuracy))\n",
    "    print \"precision: {}\".format(np.mean(precision))\n",
    "    print \"recall:    {}\".format(np.mean(recall))    \n",
    "    return (np.mean(accuracy), np.mean(precision), np.mean(recall))\n",
    "\n",
    "    \n",
    "\n",
    "#evaluate_clf(log_clf, features, labels)\n",
    "#evaluate_clf(sgd_clf, features, labels)\n",
    "#evaluate_clf(rf_clf, features, labels)\n",
    "#evaluate_clf(dt_clf, features, labels)\n",
    "evaluate_clf(ada_clf, features, labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.83967\tPrecision: 0.36563\tRecall: 0.27550\tF1: 0.31423\tF2: 0.28979\n",
      "\tTotal predictions: 15000\tTrue positives:  551\tFalse positives:  956\tFalse negatives: 1449\tTrue negatives: 12044\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tester\n",
    "from tester import test_classifier\n",
    "ada_clf = ensemble.AdaBoostClassifier()\n",
    "\n",
    "#tester.test_classifier(sgd_clf, my_dataset, features_list)\n",
    "#tester.test_classifier(log_clf, my_dataset, features_list)\n",
    "#tester.test_classifier(rf_clf, my_dataset, features_list)\n",
    "#tester.test_classifier(dt_clf, my_dataset, features_list)\n",
    "tester.test_classifier(ada_clf, my_dataset, features_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 50 folds for each of 21 candidates, totalling 1050 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/weidian1/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/weidian1/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/weidian1/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    5.3s\n",
      "/Users/weidian1/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   23.4s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   53.9s\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  1.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.5, n_estimators=50, random_state=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1050 out of 1050 | elapsed:  2.0min finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "# parameter optimisation of algorithm using GridSearchCV\n",
    "\n",
    "#trials = 1\n",
    "\n",
    "# create score function\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "def scorer_r_p(estimator, X_test, y_test):\n",
    "    y_pred = estimator.predict(X_test)\n",
    "    r_score = recall_score(y_test, y_pred)\n",
    "    p_score = precision_score(y_test, y_pred)\n",
    "#    f1_score = f1_score(y_test, y_pred)\n",
    "    if r_score<0.3 or p_score<0.3:\n",
    "        return 0\n",
    "    return  f1_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "## Create Cross Validation object for use in GridSearchCV  \n",
    "sss = model_selection.StratifiedShuffleSplit(50, random_state = 42)\n",
    "\n",
    "## State and Fit the model\n",
    "param_grid = [  {'n_estimators': [50,80,100], 'learning_rate': [0.5,1,1.5,2,3,4,5]}]\n",
    "\n",
    "\n",
    "my_clf = model_selection.GridSearchCV(ada_clf, param_grid, scoring = scorer_r_p, cv = sss, verbose=1, n_jobs=-1)\n",
    "my_clf.fit(features, labels) \n",
    "print my_clf.best_estimator_\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tester import dump_classifier_and_data\n",
    "\n",
    "### Final and parameter tuned algorithm\n",
    "clf = my_clf.best_estimator_\n",
    "\n",
    "\n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, my_feature_list)\n",
    "\n",
    "\n",
    "# https://www.kaggle.com/forums/f/15/kaggle-forum/t/4092/how-to-tune-rf-parameters-in-practice\n",
    "# /Users/andreasvoniatismbp10/Documents/Dropbox/ARTIOS/Training/git_github/version-control/ud120-projects/final_project\n",
    "# https://discussions.udacity.com/t/showing-the-best-estimator-from-grid-search-cv/194547/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
