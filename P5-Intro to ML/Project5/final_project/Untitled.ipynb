{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/weidian1/Documents/GitHub/Udacity-DAND/P5-Intro to ML/Project5/final_project\n",
      "Total number of data points:  145\n",
      "Number of POI: 18, no. of non-POI: 127\n",
      "Number of features used:  21\n",
      "salary                        0\n",
      "to_messages                  59\n",
      "deferral_payments             0\n",
      "total_payments                0\n",
      "exercised_stock_options       0\n",
      "bonus                         0\n",
      "restricted_stock              0\n",
      "shared_receipt_with_poi      59\n",
      "restricted_stock_deferred     0\n",
      "total_stock_value             0\n",
      "expenses                      0\n",
      "loan_advances                 0\n",
      "from_messages                59\n",
      "other                         0\n",
      "from_this_person_to_poi      59\n",
      "poi                           0\n",
      "director_fees                 0\n",
      "deferred_income               0\n",
      "long_term_incentive           0\n",
      "email_address                34\n",
      "from_poi_to_this_person      59\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print os.getcwd()\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "#sys.path.append(\"../pylof-master/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "#features_list = ['poi','salary',''] # You will need to use more features\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "# 异常值， 去掉total\n",
    "def remove_outerliers(data_dict,names):\n",
    "    for name in names:\n",
    "        data_dict.pop(name, 0)\n",
    "    return data_dict\n",
    "\n",
    "def dateset_summary(data_dic):\n",
    "    n_poi,n_non_poi = 0,0\n",
    "    for key in data_dict.keys():\n",
    "        if data_dict[key]['poi']==1:\n",
    "            n_poi+=1\n",
    "        else:\n",
    "            n_non_poi+=1        \n",
    "    print \"Total number of data points: \", len(data_dict)\n",
    "    print \"Number of POI: %d, no. of non-POI: %d\"% (n_poi, n_non_poi)\n",
    "    print \"Number of features used: \",len(data_dict['METTS MARK']) # randomly pick one name, get the number of features.\n",
    "\n",
    "outliers_names = ['TOTAL']#,'LAVORATO JOHN J'\n",
    "\n",
    "data_dict = remove_outerliers(data_dict,outliers_names)\n",
    "dateset_summary(dateset_summary)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_df = pd.DataFrame.from_dict(data_dict,orient='index')\n",
    "\n",
    "financial_features = ['salary', 'deferral_payments', 'total_payments', 'loan_advances', 'bonus'\n",
    "                 , 'restricted_stock_deferred', 'deferred_income', 'total_stock_value', 'expenses'\n",
    "                 , 'exercised_stock_options', 'other', 'long_term_incentive', 'restricted_stock'\n",
    "                 , 'director_fees']\n",
    "email_fatures = ['to_messages','from_poi_to_this_person', 'from_messages', 'from_this_person_to_poi'\n",
    "                 , 'shared_receipt_with_poi']  # remove 'email_address'\n",
    "def NaNs_to_0s(col):\n",
    "    return [0 if ele=='NaN' else ele for ele in col]\n",
    "\n",
    "def NaNs_to_None(col):\n",
    "    return [None if ele=='NaN' else ele for ele in col]\n",
    "\n",
    "def count_NAN(col):\n",
    "    return sum(1 for ele in col if ele == 'NaN')\n",
    "\n",
    "data_df[financial_features] = data_df[financial_features].apply(NaNs_to_0s, axis=0)\n",
    "data_df['poi'] = data_df['poi'].astype(int)\n",
    "print data_df.apply(count_NAN, axis=0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "make new feature \n",
    "1. 'email_features_miss': if email_features is missing. next step, missing email_features will be\n",
    "transformed to 0, which should by different from missing financial features.   \n",
    "2. poi_rate_to_messages = from_this_person_to_poi/to_messages  \n",
    "3. poi_rate_from_messages = from_poi_to_this_person/from_messages\n",
    "\n",
    "\"\"\"\n",
    "data_df['email_features_miss'] = data_df.apply(lambda x: 1 if x[\"to_messages\"]=='NaN' else 0, axis = 1)\n",
    "\n",
    "data_df['poi_rate_to_messages'] = data_df.apply(lambda x: 0 if x['from_this_person_to_poi']=='NaN' \\\n",
    "                                                else x['from_this_person_to_poi']*1.0/x['to_messages'], axis=1)\n",
    "\n",
    "data_df['poi_rate_from_messages'] = data_df.apply(lambda x: 0 if x['from_poi_to_this_person']=='NaN'\\\n",
    "                                                  else x['from_poi_to_this_person']*1.0/x['from_messages'], axis =1)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 full_columns_name: ['poi', 'salary', 'to_messages', 'deferral_payments', 'total_payments', 'exercised_stock_options', 'bonus', 'restricted_stock', 'shared_receipt_with_poi', 'restricted_stock_deferred', 'total_stock_value', 'expenses', 'loan_advances', 'from_messages', 'other', 'from_this_person_to_poi', 'director_fees', 'deferred_income', 'long_term_incentive', 'from_poi_to_this_person', 'email_features_miss', 'poi_rate_to_messages', 'poi_rate_from_messages']\n"
     ]
    }
   ],
   "source": [
    "features_full_list = list(data_df.columns.values)\n",
    "features_full_list.remove('email_address')\n",
    "features_full_list.remove('poi')\n",
    "features_full_list = ['poi'] +features_full_list\n",
    "data_full_dic = data_df.to_dict(orient='index')\n",
    "print (\"%d full_columns_name: %s\" % (len(features_full_list), features_full_list))\n",
    "\n",
    "#def get_train_test_dataset(my_dataset,features_list):\n",
    "#    #Extract features and labels from dataset for local testing\n",
    "#    data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "#    labels, features = targetFeatureSplit(data)\n",
    "#    # TODO: Shuffle the data\n",
    "#    from sklearn.utils import shuffle\n",
    "#    features, labels = shuffle(features, labels, random_state=1)\n",
    "#    \n",
    "#    # split train test dataset\n",
    "#    from sklearn.model_selection import train_test_split\n",
    "#    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "#    X_train = np.array(X_train)\n",
    "#    y_train = np.array(y_train)\n",
    "#    X_test = np.array(X_test)\n",
    "#    y_test = np.array(y_test)\n",
    "#    \n",
    "#    return X_train, X_test, y_train, y_test\n",
    "#    \n",
    "#X_train, X_test, y_train, y_test = get_train_test_dataset(data_full_dic,features_full_list)  \n",
    "    \n",
    "def get_features_labels(my_dataset,features_list):\n",
    "    #Extract features and labels from dataset for local testing\n",
    "    data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "    # TODO: Shuffle the data\n",
    "    from sklearn.utils import shuffle\n",
    "    features, labels = shuffle(features, labels, random_state=1)\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "    return features, labels\n",
    "\n",
    "def get_train_test_dataset(features, labels):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "  \n",
    "features, labels = get_features_labels(data_full_dic,features_full_list)\n",
    "X_train, X_test, y_train, y_test = get_train_test_dataset(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import mean,std\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import (ExtraTreesClassifier, RandomForestClassifier, \n",
    "                              AdaBoostClassifier, GradientBoostingClassifier)\n",
    "\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def scorer_r_p(estimator, X_test, y_test):\n",
    "    y_pred = estimator.predict(X_test)\n",
    "    r_score = recall_score(y_test, y_pred)\n",
    "    p_score = precision_score(y_test, y_pred)\n",
    "#    f1_score = f1_score(y_test, y_pred)\n",
    "    if r_score<0.3 or p_score<0.3:\n",
    "        return 0    \n",
    "    return r_score + p_score\n",
    " \n",
    "\n",
    "class EstimatorSelectionHelper:\n",
    "    def __init__(self, models, params):\n",
    "        if not set(models.keys()).issubset(set(params.keys())):\n",
    "            missing_params = list(set(models.keys()) - set(params.keys()))\n",
    "            raise ValueError(\"Some estimators are missing parameters: %s\" % missing_params)\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "        \n",
    "        min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        model_tree = SelectFromModel(ExtraTreesClassifier(random_state=32), prefit=False)\n",
    "        preparation = [('scaler',min_max_scaler)\n",
    "                      ,('selector',model_tree)]\n",
    "        self.preparation = preparation\n",
    "        self.params_selector ={'selector__threshold':['mean','0.5*mean','1.4*mean'] }\n",
    "        self.scores_df = None\n",
    "\n",
    "    def fit(self, X, y, cv=3, n_jobs=1, verbose=1, scoring=None, refit=True):\n",
    "        for key in self.keys:\n",
    "            print(\"Running GridSearchCV for %s.\" % key)\n",
    "            model = self.models[key]\n",
    "            #params = self.params[key]\n",
    "            print params_selector.items()\n",
    "            if isinstance(self.params[key],list):\n",
    "                params = [dict(param.items() + params_selector.items()) for param in self.params[key]]           \n",
    "            elif isinstance(self.params[key],dict):\n",
    "                params = dict(self.params[key].items() + self.params_selector.items())\n",
    "            preparation = self.preparation\n",
    "            pipe= Pipeline(preparation +[(key,model)])        \n",
    "            gs = GridSearchCV(pipe, params, cv=cv, n_jobs=n_jobs,verbose=verbose, scoring=scoring, refit=refit)\n",
    "            gs.fit(X,y)\n",
    "            self.grid_searches[key] = gs    \n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                 'estimator': key,\n",
    "                 'min_score': min(scores),\n",
    "                 'max_score': max(scores),\n",
    "                 'mean_score': mean(scores),\n",
    "                 'std_score': std(scores),\n",
    "            }\n",
    "            return pd.Series(dict(params.items() + d.items()))\n",
    "\n",
    "        rows = [row(k, gsc.cv_validation_scores, gsc.parameters) \n",
    "                     for k in self.keys\n",
    "                     for gsc in self.grid_searches[k].grid_scores_]\n",
    "        df = pd.concat(rows, axis=1).T.sort_values(by=[sort_by], ascending=False)\n",
    "\n",
    "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "        self.scores_df = df[columns]\n",
    "        return df[columns]\n",
    "    \n",
    "    def best_estimator_model(self):\n",
    "        if  isinstance(self.scores_df,pd.DataFrame):\n",
    "            model_name = self.scores_df.iloc[0,0]\n",
    "            best_estimator_ = self.grid_searches[model_name].best_estimator_\n",
    "            best_model = self.grid_searches[model_name].best_estimator_.steps[-1][-1]\n",
    "            return best_estimator_, best_model       \n",
    "        else:\n",
    "            raise ValueError('scores_df is None; run fit() and score_summary() first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for RandomForestClassifier.\n",
      "[('selector__threshold', ['mean', '0.5*mean', '1.4*mean'])]\n",
      "Fitting 1000 folds for each of 12 candidates, totalling 12000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    6.3s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-70bdbdf29fe8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mhelper1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEstimatorSelectionHelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mhelper1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'recall'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-8c9908001d32>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, cv, n_jobs, verbose, scoring, refit)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mpipe\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreparation\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrefit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_searches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/weidian1/anaconda/lib/python2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m         \"\"\"\n\u001b[0;32m--> 829\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/weidian1/anaconda/lib/python2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[1;32m    571\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m                                     error_score=self.error_score)\n\u001b[0;32m--> 573\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m                 for train, test in cv)\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/weidian1/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    766\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/weidian1/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "models1 = { \n",
    "#    'ExtraTreesClassifier': ExtraTreesClassifier(),\n",
    "    'RandomForestClassifier': RandomForestClassifier()\n",
    "#    'AdaBoostClassifier': AdaBoostClassifier(),\n",
    "#    'GradientBoostingClassifier': GradientBoostingClassifier()\n",
    "    #'SVC': SVC()\n",
    "}\n",
    "\n",
    "params1 = { \n",
    "#    'ExtraTreesClassifier': { 'ExtraTreesClassifier__n_estimators': [16, 32, 50, 100] },\n",
    "    'RandomForestClassifier': { 'RandomForestClassifier__n_estimators': [16, 32, 50, 100] }\n",
    "#    'AdaBoostClassifier':  { 'AdaBoostClassifier__n_estimators': [16, 32, 50, 100] },\n",
    "#    'GradientBoostingClassifier': { 'GradientBoostingClassifier__n_estimators': [16, 32, 50, 100]\n",
    "#                                   , 'GradientBoostingClassifier__learning_rate': [0.5, 0.8, 1.0] }\n",
    "    #'SVC': [\n",
    "    #    {'SVC__kernel': ['linear'], 'SVC__C': [1, 10]},\n",
    "    #    {'SVC__kernel': ['rbf'], 'SVC__C': [1, 10], 'SVC__gamma': [0.001, 0.0001]},\n",
    "    #]\n",
    "}\n",
    "\n",
    "\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(labels, 1000, random_state=42)\n",
    "\n",
    "helper1 = EstimatorSelectionHelper(models1, params1)\n",
    "helper1.fit(features, labels,cv=sss, scoring='recall', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
